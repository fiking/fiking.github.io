---
title: >-
  Adaption-Optimization-for-SELF:Reconciling-high-performance-with-exploratory-programming
  论文翻译
tags:
  - virtual machine
categories:
  - dissertation translation
  - virtual machine
date: 2021-08-28 22:02:27
description:
---

# 前言

  为了增加虚拟机方面的理论知识，开始了虚拟机相关的论文翻译和学习。先制定个伟大的目标，前期每周一篇翻译，后续在根据学习情况进行论文的总结精炼。

  开篇文章为 **Urs Hölzle** 1994年的博士论文《Adaption Optimization for SELF: Reconciling high performance with exploratory programming》。根据网上资料此文章是 OpenJDK、V8的主要思想来源，所以优先进行阅读。

 <!-- more -->

# 翻译

## 论文题目

SELF 自适应优化：探索性编程与高性能共存

## 作者

Urs Hölzle

## 时间

1994年八月

## 正文

### 摘要

面向对象语言带来许多的益处，其中的抽象性，可以让程序员隐藏来自对象客户端的对象实现细节。不幸的是，在跨越抽象边界如频繁的过程调用中总是伴随着巨大的运行时开销。因此，尽管频繁的使用抽象是这类语言的设计目的，但这是不切实际的，因为，它会导致程序非常低效。

激进的编译器优化会降低抽象的开销。但是优化编译器带来长的编译时间会延长编程环境反馈程序变化的时间。此外，优化也会与源码级别的调试冲突。因此，程序员面临一个两难的选择，不得不在高效的抽象和高效的编程环境之间进行选择。本论文会展示如何通过延迟编译调和这个看起来是矛盾的目标。

融合四个新的技术来达成高性能和高响应：

- ***Type feedback***  通过允许编译器基于获取自运行时系统的信息内联消息发送（message send）来取得高性能。平均来说，在有类型反馈的SELF系统上，程序运行会比在之前的 SELF 系统快1.5倍；而与商业的 Smalltalk 实现对比，两个中型的 benchmark 会快三倍。这个水平的性能是从一个比之前 SELF 编译器更简单和更快速的编译器上取得的。
- ***Adaptive optimization*** 在取得高响应的同时不会牺牲性能，通过使用非优化编译器生成初始代码，同时在运行期间使用优化编译器重编程序中重度使用的代码部分。在前一代的工作平台上如 SPARCstation-2，50 分钟的交互中有将近200个暂停超过200ms，21个暂停超过了 1 秒。在当代的工作平台上，只有13个暂停超过400ms。
- ***Dynamic deoptimization*** 在需要时通过隐式地重建非优化代码，使得程序员可以远离调试优化过的代码的复杂性。不管一个程序是否被优化过，它都可以被停止，检查和单步调试。比起之前的方法，去优化（deoptimization）支持更多的调试的同时，对于可执行的优化代码添加更少的限制。
- ***Polymorphic inline caching*** 实时生成类型信息序列来加速来自于相同调用位置但具有不同的类型对象的消息发送。更重要的是，它会为优化编译器搜集了具体的类型信息。

具有高性能和良好的交互行为的这些技术，让探索性编程既适用于纯粹面向对象语言，也适用于需要更高最终性能的应用领域；调和了探索性编程，繁多的抽象和高性能。

### 致谢

作者的感谢，暂不翻译。

### 目录列表

暂不翻译。

### 图片列表

暂不翻译。

### 表列表

暂不翻译。

### 1. 引言

面向对象编程越来越流行，因为它让编程更简单了。它允许程序员隐藏来自对象客户端的实现细节，将每个对象放入对应的抽象数据类型中，操作和状态都只能通过消息发送进行访问。延迟绑定极大地提高了抽象数据类型的能力，通过允许相同抽象数据类型的不同实现，在运行时可以实现进行互换。这个互换指的是一段调用某个对象的一个操作的代码并不总是最终会被执行调用的代码：延迟绑定（也叫动态分发）会选择一个操作合适的实现基于对象的准确类型。因为延迟绑定对于面向对象是最基本的，需要高效的实现来支持。

理想状态下，面向对象语言会全部使用延迟绑定的，即使对于实例变量访问这种基础的操作也是一样。越普遍地使用封装和动态分发，最终的代码具有越高的自由度和重用性。但是延迟绑定会带来效率问题。例如，如果所有的实例变量方位都使用消息发送，编译器无法将一个对象的属性x的访问翻译为一个简单的加载指令，因为对象间的x实现可能是不同的。例如，一个笛卡尔点可能只是返回一个实例变量的值，而一个极坐标点会根据半径和角坐标计算x的值然后进行返回。这种变化恰恰是延迟绑定的意义：x的操作与操作的实现（实例变量的访问和计算）的绑定延迟到了运行的时候才做。因此，x的操作必须编译成一个动态分发的调用（也叫间接函数调用或者虚函数调用），从而可以在运行时选择一个合适的实现。这导致了一个周期的指令变成了十个周期的调用。随着源码自由度和重用度的增加会带来显著的运行时开销；从而看起来封装和高效是不能共存的。本文会展示如何平衡两个。

一个相似的效率问题也出现在渴望使用探索性编程环境上。一个探索性编程环境增加了程序员的生产率通过对所有的编程操作给与立即的反馈；零暂停交互允许程序员集中于手上的任务而不被长时间的编译暂停分心。通常来说，系统设计者在探索性编程环境会使用解释器和非优化的编译器。不幸的是，解释器的开销，伴随着动态分发的效率问题，降低了执行性能，因此限制了这种系统的使用。这篇论文描述如何降低动态分发的开销从而保证一个交互探索编程环境的反馈获取。

我们研究的轮子是面向对象语言 SELF。SELF 的纯语义加剧了面向对象语言面临的实现问题：在 SELF，每个单操作（甚至赋值操作）都涉及延迟绑定。因此，这个语言是降低延迟绑定优化的理想测试用例。同样重要的是，SELF是为了探索编程设计的。每个探索编程环境必须提供程序变化之后快速的转化，和局部程序的简单调试，从而增加程序员的生成率。因此，一个这种系统的优化编译器不仅要解决动态分发带来的性能问题，也要适配交互性：编译必须快速和非侵入性的，还有系统必须支持全时间段的全源码调试。

我们为SELF实现了一个系统，工作的贡献有：

- 一个新的优化策略， ***Type Feedback***，允许任意的动态分发调用被内联。在我们为 SELF 做的实现样例中，类型反馈将函数调用降低了四倍同时提高了 70% 的性能比起没有类型反馈的系统。尽管 SELF 和 C++ 是完全不同的语言模型，新的系统使得 SELF 在两个中型的面向对象的程序达到了优化过的C++性能的一半。
- 一个重编译系统，动态重新编译应用的 "hot spots"。这个系统通过一个快速的非优化编译器产生初始化代码，然后通过一个慢的优化编译器重新编译那些时间紧迫的部分代码。引入自适应动态编译戏剧性地提高了 SELF 系统的交互性能，让优化编译和探索性编程环境结合成为了可能。即使在前一代工作平台如 SPARCstation-2，50 分钟的交互中有将近200个暂停超过200ms，21个暂停超过了 1 秒。
- 一个内联缓存的扩展， ***polymorphic inline caching***，可以加速多态   调用点的动态分配调用。此外多态内联缓存可以做为类型反馈的类型信息源，平均提升11%的性能。
- 一个调试系统，动态退优化为全局优化代码提供源码级别的调试。即使优化代码只支持严格的调试，这个系统可以隐藏那些限制，提供全源码级别的调试（例如，单步调试这类调试操作）。

虽然我们的实现基于动态编译，但是本文中大部分的技术不需要它。类型反馈会直接被整合进传统的编译系统（看5.6节），类似于其它的基于 profile 的系统。源码级别的调试可以通过保留编译前的未优化代码到一个分离的文件来实现（看10.5.3）。多态内联只需要一个简单的桩生成器，并不需要全成熟的动态编译。只有5.3节描述的动态编译系统--从本质上而言--才需要动态编译。

本文描述的技术都不仅仅限定于 SELF 语言。就如10.5.3节的讨论，调试系统具有非常大的语言无关性。类型反馈可以优化任何语言的延迟绑定；如和面向对象语言相比，那些有重度使用延迟绑定的非面向对象语言（如，APL的通用操作符，Lisp的通用算术）也会从这种优化中获得收益。最后，任何使用动态编译得系统都可能从自适应编译中获取提高性能和降低编译暂停得收益。

本论文剩余部分，我们会描述这些技术得设计和实现，并且评估它们对于 SELF 系统的性能影响。所有的技术都全部实现并且足够稳定地成为了公共 SELF 发布包的一部分。第二章呈现 SELF 的概述和随后章节描述的工作概述，以及讨论相关工作。第三章讨论如何动态分发可以被运行时系统优化，例如，编译器外。第四章描述非优化的 SELF 编译器和评估它的性能。第五章描述类型反馈如何降低动态分发的开销，第六章描述 SELF 优化编译器的实现。第七章和第八章评估新的 SELF 编译器对于之前SELF系统的性能，对其它语言的性能，以及调查硬件特性对性能的影响。第九章讨论优化编译器怎么对系统交互行为产生影响。第十章描述我们的系统怎样提供全源码级的调试且不影响编译器的优化性能。

如果你非常的匆忙且已经很熟悉之前的 Smalltalk 或者 SELF 的实现，我们建议你可以跳过第二章（简介），然后阅读每一章末尾的总结，加上结论。

143页的词汇表包含了本文大部分重要术语的简短定义。

### 2. 背景和相关工作

这个章节展现本论文的一些背景和相关工作。首先，我们简短的介绍 SELF 语言和它的目标，其次是 SELF 系统的概述。然后，我们的新系统的编译过程，最后我们会回顾相关的工作。

#### 2.1 SELF 语言

SELF 是一个 动态类型，基于原型和面向对象的语言，最初的设计是1986年 David Ungar 和  Randall B. Smith 在 Xerox PARC 做的。想要作为 Smalltalk-80 编译语言的一个备选，SELF尝试最大化程序员在探索性编程环境的生产力通过保持语言的简单性和纯净，而不降低表达能力和扩展性。

SELF 是一种纯粹的面向对象语言：所有的数据都是对象，且所有计算的执行都是通过动态绑定消息发送的（包括所有的实例变量访问，即使在接收者对象里）。因此，SELF合并状态和行为i：语法上，方法调用和变量访问是不区分的--发送者不知道消息是简单数据访问的实现还是方法的实现。于是，所有的代码时表达独立的，因为相同的代码可以被不同结构的对象重复使用，只要这些对象正确实现期望的消息协议。换句话说，SELF支持全抽象数据类型：只有对象的接口是可见的，所有的实现细节如对象的大小和结构都被隐藏，即使代码在对象本身里。

SELF 其它主要的重点如下列表：

- SELF 是动态类型的：程序包含无类型的定义。
- SELF 是基于原型的而不是类。每个对象是自描述的可以被独立改变。除了这种方法的灵活性，基于原型的系统可以避免元类（metaclasses）带来的复杂度。
- SELF 有多继承。多继承设计在这些年经历了一些改变。SELF-87 只有单继承，但是 SELF-90 引入了 优先多继承结合一个新的隐私机制为了更好的封装和一个“发送者路径判定”规则为了消除相同优先级父节点的歧义。最近，钟摆又摆回了简单性：SELF-92 消除了发送者路径判定因为它倾向于隐藏歧义，而 SELF-93 消除了优先级继承和来自语言的隐私。
- 所有的控制结构式用户定义的。例如，SELF没有 **if** 语句。作为替代，控制结构通过消息发送和块（闭包）实现，就像 Smalltalk-80。但是，不同于 Smalltalk-80的是，SELF的实现没有固化任何的控制结构，换句话说，程序员可以改变任何方法（包括那些实现if语句，循环和整型加法的方法）的定义，系统会忠实地反射这些变化。
- 所有的对象是堆分配的且被垃圾回收器自动解除分配。

这些特性被设计来发挥现代计算机的计算能力，使得程序员的生活更简单。例如，表达独立性使得重复使用和重组代码变得简单，但是产生了实现问题，因为每个数据访问都关联了一个消息发送。比起关注最小程序窒执行时间，SELF更关注于最小编程时间。

#### 2.2 SELF系统

SELF系统的实现目标反映了语言：最大化编程生产效率的特点。几个特性为这个目标做出贡献：

- ***Source-level  sematics***。系统的行为总是可以用源语言级别的术语解释。程序员应该从不面对一些如 “segmentation fault"，”arthmetic exception: denormalized float“ 的错误，因为这些错误在缺少对底层实现细节的深入了解是无法解释的，这些实现细节超出了语言的定义因此难以理解。因此，所有的 SELF 原语都是安全的：算术操作测试溢出，数组访问执行下标边界检查等。
- ***Direct execution sematics(interpreter semantics)***。 系统应该总是表现如同直接执行方法源码：任何源码改变都是立即生效的。直接执行语义使得程序员免于担心不得不显式调用编译器和链接器，免于不得不处理编译依赖等枯燥的细节。
- ***Fast turnaround time***。 在做出变化之后，程序员不会因为慢速编译器和链接器而等待；这类的延迟应该尽可以能得短（理想下，只是几分之一秒）。
- ***Efficiency***。最后，程序应该总是有效率的运行不受 SELF纯度的影响：程序员不应该因为选择 SELF 而不选择更传统的语言而受惩罚。

这些目标不是轻易能达成的。特别是，因为强调成为程序员正确的语言而不是成为机器的语言，SELF 是很难实现的有效率的。几个关键的语言特性创造了特别难的问题：

-  由于所有的计算都是由消息发送执行的，在底层实现中调用频率极其高。如果简单和通用的计算，这些在传统的编译器中通常只需要单条指令（如一个实例变量访问或者一个整形加法），都调用动态分发的过程调用，程序将会比传统语言运行慢上许多，即使没有一百的慢。
- 相似的，由于没有内置的控制结构，一个简单的 **for** 循环涉及数十条消息发送和几个块（闭包）的创建。因为控制结构的定义可以被用户改变，编译器不能走捷径地将它们的翻译和手写优化代码模式硬关联在一起。
- 因为总体目标是最大化程序员生产率，系统应该有高频的交互和提供立即的反馈给用户。因此，长的编译暂停（出现在优化编译器中的）是无法接受的，进一步限制了实现者在找寻有效的 SELF 实现的自由。

下面的章节给出当前SELF实现的概述和它是怎样尝试解决上面描述的问题的。在描述基础系统之后，我们会强调新的解决方案，这是这篇文章的主题，以及它们怎么适配已经存在的系统。

##### 2.2.1 实现概述

 SELF 虚拟机有如下几个子系统组成：

- 一个 ***memory system*** 处理分配和垃圾回收。对象被存放在堆中；一个分代的扫描垃圾回收器回收不使用的对象。对象引用通过两个比特位标签标记在每个32位字的低两位上（标签00 是整形，01 是指针，10 是浮点，11是对象头）。所有的对象处理整形和浮点由至少两个字组成：一个字表示header，一个指针指向对象 map。一个 map 描述对象的格式，可以被看成是对象低级别类型。有相同格式的对象共享相同的map，所以对象的布局信息只会被存储一次。为保持语言规定的自描述对象的抽象，map是写时拷贝：l例如，当一个槽被添加到一个对象时，这个对象会得到一个新的map。
- 一个 ***parser***  读取文本描述的对象，以及将其转化为真的对象存储到堆。方法被表示为简单的字节码集合（本质上是，“send"，”push literal“，和”return“）。
- 给予一个消息名和一个接送者，***lookup system***  决定查找的结果，例如，匹配槽。查找首先检查哈希表（代码表）找寻编译过的代码是否存在。如果存在，代码就被执行；否则，系统执行实际的对象查找（如果需要的化遍历接受者的父对象），然后调用编译器生成机器码来执行方法或者数据访问。
- ***compiler***  转换一个方法的字节码为机器码然后存储到***code cache***。 如果代码缓存中已经没有空间给新的编译过的方法，已经存在的方法会被清理来获得空间。代码缓存保存近似 LRU 信息来决定哪个编译过的方法被清理。
- 最后，虚拟机也包含许多的原语，可以被 SELF 程序调用来执行算术计算，I/O，图形绘制等。新的原语可以在运行时被动态链接到系统。

参考[88]，[115]，和[21]包含更多的系统细节。

##### 2.2.2 效率

因为 SELF 的纯语义威胁让程序效率极其低下，许多早期的实现尽力使用编译技术优化 SELF 程序。这些技术中有些是非常成功的：

***Dynamic compilation*** 。 编译器按需动态的翻译源方法成编译过的方法。这意味着没有分离的编译过程，执行和编译是交错的。（SELF 动态编译的使用灵感来自 Deutsch-Schiffman Smalltalk system[44]。）

***Customization***。 定制化允许编译器决定一个方法中许多消息接送者的类型[23]。它利用方法的许多消息是发送给 **self** 的事实来扩展动态编译。编译器根据每个接送者类型为一份源码创建不同版本的编译代码（Figure 2-1）。例如，方法min：源方法计算两个数的最小值，会被创建为两个不同的方法。这个复制允许编译器客制化每个版本到特定的接受者类型。特别是，在编译时知道 self 的类型使得编译器可以内联所有的发送 self。客制化在 SELF 中是非常重要的，因为许多消息是发送给 self，包括实例变量访问，全局变量访问，和许多用户定义的控制类型。

{% if 1 == 1 %} 
  {% asset_img figure_2_1.png title %}
{% else %}
  ![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_2_1.png)

{% endif %}

***Type Prediction***。确定的消息几乎只发送给特定的接受者类型。对于这样的消息，编译器使用最早由 Smalltalk 系统引入的优化[44，132]：它预测接受者的类型基于消息名字并且插入运行时类型检测于消息发送前用于测试期望的接送者类型（Figure 2-2）。类似的优化在 Lisp 系统中被用于优化通用计算。沿着类型测试成功的分支，编译器有关于接受者类型的精确信息可以静态地绑定和内联一个消息的拷贝。类如，现有的 SELF 和 Smalltalk 系统预测 ‘+’ 会被发送给整形 [128，58，44]，因为测量显示 这个类型 90% 的时间会出现 [130]。如果测试的开销是比较低且成功的可能性比较高，则类型预测会提高性能。

{% if 1 == 1 %} 
  {% asset_img figure_2_2.png title %}
{% else %}
  ![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_2_2.png)

{% endif %}

***Splitting*** 是另外一种方法，将一个多态的消息转变为多个分离的单态消息。通过拷贝部分的控制流图 [23，22，24] 避免类型测试。例如，一个对象在 if 语句的一个分支上是一个整形，而在另一个分支上是浮点（Figure 2-3）。如果这个对象是 if 语句之后的消息发送的接收者，编译器可以拷贝这个发送到两个分支。因此在每个分支上都是确定的接受者类型，编译器可以分别内联这两个发送的拷贝。

{% if 1 == 1 %} 
  {% asset_img figure_2_3.png title %}
{% else %}
  ![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_2_3.png)

{% endif %}

合并在一起，这些优化将 SELF的性能提升到合理的水平。举个例子，Chambers 和 Ungar 报告了 SELF 显著地跑赢了 ParcPlace Smalltalk-80 的实现在一个小的类C的整形测试用例集上[26]。

##### 2.2.3 源码级的语义

语言和实现特性的结合确保了所有程序的行为，即使错误的部分，也可以通过源码级别的术语独立理解。语言保证了每个消息发送要么找到一个匹配槽（访问它的数据或者运行它的方法），或者导致一个“message not understood" 的错误。最后，这个级别的唯一错误是查找错误。此外，实现保证了所有的原语是安全的。因为所有的原语会检查它们的操作数和结果，如果操作数无法被执行会以明确的方式失败。例如，所有的整型算术原语检查溢出，数组访问原语检查范围错误。最后，因为 SELF 不允许指针算术和使用垃圾回收，所以系统是指针安全的：不会偶然地复写随机的内存区域或者解引用悬挂指针。这些特性的结合让它更容易找到程序错误。

安全原语让高效实现更难[21]。例如，每个整型加法不得不检查溢出，从让它变慢。更重要的是，整型操作的返回值类型是未知的：所有的原语都有 “故障块” 的参数，如果操作失败，一个消息会被发送给这个参数。然后这个发送的结果会变成原语调用的返回值。例如，当前 SELF 系统的整型的 “+” 方法调用 IntAdd：带有失败块的原语会将失败块参数转化为任意精度的整型然后进行相加。因此表达式 x + y 的精确的返回值类型是未知的，即使 x 和 y 都是已知的整型：没有溢出，返回值会是整型，但是当结果太大而不能被表示为一个机器级别的整数时，结果就会时任意精度的整数。因此，即使 x 和 y 是已知的整数，编译器也无法静态的知道表达式 x + y + 1 的第二个 “+” 会调用整型的加法还是任意精度数的加法。安全原语帮助程序员的同时也潜在地使执行变慢。

##### 2.2.4 直接执行语义

SELF 通过使用动态编译模仿解释器。每当一个没有相关编译过代码的源码方法被调用，编译器会被自动调用生成缺失的代码。反过来，每当用户改变源方法，所有依赖于旧定义的编译过代码会被无效化。为了实现这个系统在源码方法和编译过的方法维持一个依赖链接[71,21]。

因为没有显式的编译和链接步骤，传统的编写-编译-链接-运行循环被简化为一个编写-运行循环。程序可以在运行时被改变，所以应用可以被在线调试而不需要从头开始运行。

#### 2.3 编译过程概述

Figure 2-4 较详细地展示了新的 SELF-93 系统的编译过程。SELF 的方法源码以对象的形式存储到堆上，就像其它的数据对象一样。方法的代码被编码为一个简单栈式机器的一串字节码（“instruction”）。这些字节码可以在解释器上直接执行；但是当前的 SELF 系统从没有这样做。反而，这些字节码总是按需地被翻译为机器码。当一个源码第一次被执行，机器码被实时生成。通常，第一次编译的代码是由“快速但笨"的编译器生成的。未优化的代码是字节码的直接翻译所以相当的慢；章节4 更细节地描述非优化的编译器。

{% if 1 == 1 %} 
  {% asset_img figure_2_4.png title %}
{% else %}
  ![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_2_4.png)

{% endif %}

未优化的代码包含一个调用计数器。每当计数器超过一定的阈值，系统会调用重编译系统来决定是否优化是需要的以及应该重新编译哪些方法。然后重编译系统调用优化编译器。章节5详细的解释了重编译过程，章节6描述优化编译器。

作为我们工作的一部分，我开发了一个新的内联缓存的变种，多态内联缓存（polymorphic inline caches(PICs)）。PICs 不仅加速多态调用点的运行时分配，也为编译器提供接收者类型的值信息。章节3描述PICs怎么工作的，章节5解释了优化编译器怎么利用 PICs 中的类型反馈信息。

在一些例子中，优化方法会再被重新编译。例如，重编译系统可以因为第一次优化后的方法仍然包含许多可以被内联的调用（比如，第一次优化编译的时候没有足够多的类型信息而不能内联）而决定重新编译。一个优化过的方法也会被重新编译当它遇到 “uncommon cases"：编译器可能预测某些场景从不会出现（如，整型溢出）而忽略了这种场景的代码生成。如果这种被忽略的场景刚好出现，原始优化过的方法就会被重新编译，扩展成处理这些特殊场景的代码（章节6.1.4）。

在本文的剩余部分，我们会按通常的顺序讨论系统的每个组件：首先，多态内联（PICs）和非内联编译器，然后重编译系统，最后是优化编译器本身。

#### 2.4 本工作的收益

本文所描述的所有技术都在 SELF-93 系统中实现了，在如下几个方面有了提升：

- ***Higher performance on realistic programs***。通过自动重编译和程序关键性能段的重新优化，我们的优化编译器可以利用运行时收集的信息。这些额外的信息经常使得编译器产生比之前代码更有效率的代码，即使编译器只执行很少的编译时程序分析。例如，DeltaBlue 约束求解器，当前的代码比之前版本的优化编译器产生的代码快三倍（章节7）。
- ***More stable performance***。新的编译器更多的依赖于动态观察到的程序行为和更少的静态分析，因此不太会受到静态分析技术盲点的影响。脆弱的性能是前一代 SELF 编译器的主要问题：小的源文件改变会导致戏剧性的性能损失，因为改变可能会导致一个特别重要的优化失效。例如，Stanford ineger  benchmarks 的一组小改变在前一代 SELF 编译器编译下会减慢 2.8 倍，而在新的编译器下编译只降低 16%（见章节7.4）。
- ***Faster and simpler compilation***。我们的方法依赖于运行时系统的动态反馈而不是使用复杂的静态类型分析技术在更好的运行时性能上有额外的优势。首先，它导致一个简单的编译器（11000 对比 26000 非注释源行代码）。第二，新的编译器比之前的编译器快2.5倍（见9.5节）。对于用户，编译暂停降低了更多（见9.2节）因为系统只是优化一个应用的关键路径。
- ***Support for source-level debugging***。在之前的 SELF 系统，用户可以打印优化程序的栈，但是他们不能在运行的时候改变程序，还有他们也不能执行诸如单步调试等通用的调试手段。我们的新系统提供随时的退优化功能让调试员的工作更简单（见第十章节）。

#### 2.5 相关工作

本节比较 SELF 系统和常见的相关工作；后续章节给出更多的比较细节。

##### 2.5.1 动态编译

SELF 使用动态编译，例如，运行时实时生成代码。动态生成编译代码而不是使用传统的批量式编译的思路源自于快速解释器的需求不断增加；通过编译到本地代码，一些解释器的固定开销（特别是伪指令的解码）可以被避免。例如，假设变量的类型保存不变，Mitchell[97] 建议可以将动态类型解释式程序的一部分转为编译形式。编译过的代码第一次被生成是作为一个表达式解释时的副作用产生的。类似地，线程代码[12]最早被用于消除一些解释过程的固定开销的。

传统上使用解释器或者动态编译器有两个原因：第一，一些语言很难被有效的静态编译，通常是因为独立的源程序没有足够的低级别的类型实现信息来生成高效的代码。第二，一些语言过于强调交互使用因此被实现为解释器而不是慢速编译器。

APL 是一种既难于进行静态编译（因为许多操作是多态的）且强调交互使用的语言。不出意外的话，APL 系统是最早探索动态编译的系统之一。例如，Johnston[80] 描述了一个使用动态编译作为解释的一个高效替代的 APL 系统。这些系统使用的一些机制类似于客制化（如，Guibas and Wyatt[61]）和内联缓存（Saal and Weiss[111]）。

Deutsch 和 Schiffman 在面向对象语言开创性地使用了动态编译。他们的 Smalltalk-80 的实现动态翻译了 Smalltalk 虚拟机定义的字节码到本地机器码并进行缓存供后续使用；Deutsch 和 Schiffman 估计仅仅使用简单的动态编译替换解释就加速了他们的系统 1.6 倍，使用更复杂的编译器则会有将近 2 倍的收益。

Franz[55] 描述了一个在加载时将紧凑的中间代码表示转为机器码的动态编译的变种。就像 Smalltalk ”snapshot“ 中的字节码，中间表示代码是架构独立的，同时，它也尽量是语言独立的（当前的实现只有Oberon支持）。从中间码到机器码的编译是足够快的使得当前加载器与常规加载器相比差异不大。

动态编译除了语言实现之外对于应用也是有用的[82]，它已经在多种方式下被使用。例如，Kessler et al. 用它来实现调试器的快速断点[84]。Pike et al. 通过动态生成最佳代码序列来加速 "bit-blt" 图形原语[103]。在操作系统中，动态编译被用来高效支持细粒度并行[32，105]和消除协议栈的开销[1]，以及动态链接[67]。动态编译也在其它领域被使用，如数据库查找优化[19，42]，微代码生成[107]，快速指令集仿真[34，93]。

##### 2.5.2 客制化

客制化部分编译代码到特定环境的想法与动态编译密切相关因为环境信息直到运行前不总是有效的。例如，Mitchell 的系统[97] 特殊化算术操作到操作数的运行时类型。当变量的类型改变了，所有的依赖于这个类型的编译代码都会被丢弃掉。因为这个语言不支持用户自定义多态且不是面向对象，这个方案的主要动机是降低解释过程的固定开销和将一些通用的内置操作替换为简单的，特定的代码序列（如，用整型加法替换通用加法）。

相似地，APL 编译器为某些表达式创建特定的代码[80，51，61]。对于这些系统，HP APL 编译器[51]最接近 SELF 使用的客制化技术。HP APL/3000 系统按一条一条语句的方式编译代码。除了执行 APL 特定的优化，编译过的代码被特定化根据特定的操作数类型（维数，每一维的大小，元素类型，存储布局）。这些所谓的”硬“代码比起通用的版本可以更高效地执行因为一个APL操作执行的计算可能会因实际参数类型而有很大差异。为了保持语言的语义，在编译表达式的时候，特殊代码之前会有一段验证导言（prologue）保证参数类型确实符合之前给出特定的假设。因此，编译生成的代码可以在之后的表达式执行中安全地重复使用。如果类型是相同的（希望这是常见的情况），无需在进一步编译；如果类型不同，一个新的更少约束性假设的版本会生成（所谓的“软"代码）。从系统的描述中，老的”硬“代码是否会保留是不清楚的，多个硬版本是否可以同时存在也是不清楚的（不同场景的特定化版本）。

客制化也被用于传统的，面向批处理的编译器。例如，Cooper et al. 描述一个 FORTRAN 编译器会创建程序的客制化版本来使能某个循环优化[36]。Nicolau 描述了一个 FORTRAN 编译器会动态地选择一个合适的静态生成的循环版本[99]。Saltz et al. 延迟循环调度直到运行时[112]。Przybylski et al. 为不同的模拟参数集合生成特定的缓存模拟器[104]。Keppel et al. 讨论为不同的应用生成值特化的特定化运行时[83]。在许多面向理论的计算机科学领域，客制化被称为局部求值[15]。

##### 2.5.3 先前的 SELF 编译器

本论文描述的 SELF 编译器有两个前代。第一个 SELF 编译器[ 22，23] 引入客制化和分离，取得相当不错的性能。对于 Stanford integer benchmarks 和 Richards benchmark，程序运行比优化过的 C 慢 4-6 倍，同时比最快的 Smalltalk-80 系统快两倍。编译器的中间码是基于树的，这让它难以显著地提高代码质量因为没有显式的控制流表示。例如，难以找到给定节点的后继（控制流术语），这让跨几个节点的优化难以实现。编译速度通常很好但会有很大的波动因为有些编译器算法的编译时间与源代码大小不是线性的。但是，编译暂停很明显，因为所有的源码都会被全量优化。这个编译器用了 9500 行 C++ 实现。

第二个编译器[21,24]（叫 SELF-91）被设计来移除第一个编译器的一些限制且进一步提高了运行时性能。它引入了迭代类型分析和一个更复杂的后端。因此，它在 Stanford integer benchmarks 上取得显著的性能（达到了优化过的C的一半性能）；但是，在 Richards benchmark 上的性能没有显著地提高。不幸的是，用户也在他们的程序上遇到相同的差异：一些程序（特别是小整型循环）表现为良好的性能的同时，许多大程序表现并不好（我们会在7.2节详细地讨论这个差异的原因）。编译器执行复杂的分析和优化也带来了编译时间的开销：比起前一代 SELF 编译器，编译慢了几倍。且当前的编译暂停会导致更多的用户不愿意使用新的编译器。比起前一代的编译器，SELF-91 编译i器相当复杂，用了 26000 行 C++代码实现。

##### 2.5.4 Smalltalk-80 编译器

Smalltalk-80 可能是最接近 SELF 的面向对象的语言了，有几个项目在调研加速 Smalltalk 程序的技术。

###### 2.5.4.1 Deutsch-Schiffman 系统

Deutsch-Schiffman 系统[44]代表着最先进的商业 Smalltalk 实现。它包含了一个简单但快速的动态编译器只执行窥孔优化没有内联。但是，不像 SELF，Smalltalk-80 的实现硬编码了某些重要的方法，如整型加法，消息实现的 if 语句，和某些循环。因此，Deutsch-Schiffman 编译器可以为这些结构生成高效的代码，否则只能通过类似于 SELF 编译器做过的优化来实现。Deutsch-Schiffman 编译器大约使用 50 条指令来生成一条编译过的机器指令[45]，因此编译器暂停几乎不可见。

除了动态编译，Deutsch-Schiffman 系统还开创了几项优化技术。内联缓存通过缓存最后的查找结果加速消息查找（见第三章）。因此，许多发送的开销可以降低到一个调用的开销和一个类型测试的开销。SELF 也使用内联缓存，且多态内联缓存扩展了它的用处到多态调用点（见第三章）。类型预测加速了普通发送通过预测可能的接送者类型（见 2.2.2节）。所有的 SELF 编译器除了第四章描述的非优化的 SELF 编译器都不同程度的使用了类型预测。优化的 SELF 编译器通过基于类型反馈信息的动态预测接收者类型扩展了静态类型预测（第五章）。

###### 2.5.4.2 SOAR

SOAR("Smalltalk On A RISC") 项目通过软硬协同的方式加速了 Smalltalk[132，129]。在软件方面，SOAR 使用了非动态的本地代码编译器（如，所有的方法在解析之后都会被编译成本地代码），内联缓存，类型预测，和一个分代扫描垃圾回收器[131]。就像 Deutsch-Schiffman 编译器一样，SOAR 编译器不执行昂贵的全局优化或者方法内联。在硬件方面，SOAR 是 Berkeley RISC II 处理器的变种[98]；最重要的硬件特性是寄存器窗口和标记整型指令。比起其它的 Smalltalk 在 CISC  上的实现，SOAR 软硬件特性的结合是非常成功的：在一个 400ns 的时间周期里，SOAR 与 70ns的微指令 Xerox Dorado 工作站，但比跑在周期时间为 200ns 的 Sun-3 上的 Deutsch-Schiffman Smalltalk 系统快 25%。但是，我们会在第八章中看到SELF系统使用的优化技术极大地降低了特殊支持硬件的性能收益。

###### 2.5.4.3 其它 Smalltalk 编译器

其它的Smalltalk 系统尝试通过标注类型声明来加速程序。Atkinson 部分实现了一个 Smalltalk 编译器，其使用类型声明作为提示来生成更好的代码[11]。例如，如果程序员定义一个局部变量为 “class X" 类型，编译器会查找这个变量的消息发送然后内联调用方法。为了代码安全，编译器在内联代码之前插入了类型检查；如果当前的值不是 class X 的实例，一个非优化的，无类型的方法会被调用。虽然，Atkinson 的编译器一直不是完全的，它可以运行小的用例得到与 Sun-3 上的 Deutsch-Schiffman 系统相比，速度提高了大约两倍。当然，为了获得加速，程序员不得不小心地在代码里标注类型，只有调用使用了正确的类型才会被加速。

TS 编译器[79]使用了类似的方法。类型被指定为一组类，当接送者类型是单类编译器可以静态绑定调用。方法可以单一内联如果程序员标记它们是可以内联的。如果接受者类型是一小组类，编译器为每个类插入类型检查然后分别优化各个分支。这些优化可以被看做是类型预测的一种形式，它是基于程序员提供的类型声明，而不是编译器硬连接的消息列表和预期类型。但是，不像类型预测和Atkinson 的编译器的是，程序员的类型声明不是一种提示而是一种坚定的承承若（这些承诺的有效性通过类型检查器来检查，但是检查器一直式不完善的，因此无法分析 Smalltalk 系统的重要部分）。

TS 编译器的后端使用 RTL-base 中间形式进行粗放的优化。因此（还有它本身是用 Smalltalk 编写的），编译器非常的缓慢，编译一个非常小的 benchmark 需要 15 到 30 秒[79]。有效的关于高效代码生成的公开发布的数据非常少，因为 TS 编译器从没有完整过，只能编译非常小的 benchmark 程序。在 68020-base 的工作站上关于 TS 和 Tektronix Smalltalk 解释器的比较数据显示对于小 benchmark ，TS 是 Deutsch-Schiffman 系统的两倍快，这是标注了类型声明的TS[79]。但是比较不是完全有效的因为 TS 没有实现一些 Smalltalk结构的完全语义。例如，整型计算没有溢出检查。这个对于小的 TS benchmark 的性能会有显著影响。例如，sumTo benchmark的循环（从1到10000一次相加）只包含很少的指令因此溢出检查会有显著的开销。更重要的是，累积求和的结果的类型会无法用 SmallInteger 表示，因为 SmallInteger 运算的结果不一定就是 SmallInteger（当溢出发生，Smalltalk-80 原始故障代码将参数转换为任意长度的整数并返回这些参数的总和）。因此，求和的类型声明（上边界）不得不更通用，这个会显著降低生成的代码的运行速度。

### 3. 多态内联缓存

面向对象的程序发送许多的消息，因此发送必须很快。本章首先回顾动态类型语言里已经存在的知名的提高查找效率的技术，然后描述多态内联缓存，这是我们开发的一个标准内联缓存的扩展版本。

#### 3.1 离线查找缓存

面向对象语言用消息发送替换过程调用。发送动态绑定消息比起调用静态绑定过程花费更多因为程序必须根据接收者的类型和语言的继承规则查找正确的目标函数。虽然早期的 Smalltalk 系统有简单的继承规则和相对慢速的解释器，方法查找（也叫消息查找）仍然占据执行时间的大部分。

***Lookup caches*** 降低了动态消息绑定的固定开销。一个查找缓存映射了一组（接受者类型，消息名）到目标方法且保存了大部分最近使用的查找结果。消息发送首先用给定的接收者类型和消息名为索引查询缓存。当缓存探测失败它们才会调用（昂贵的）查找函数通过遍历继承图来查找目标函数，然后存储结果到查找缓存，可能会将比较老的查找结果替换掉。查找缓存能有效的降低查找的固定开销。例如，Berkeley Smalltalk，没有查找缓存会慢 37% [128]。

#### 3.2 调度表

静态类型的语言总是通过调度表实现消息查找。一个通用的变种是使用消息名（编码为整数）来索引特定类型消息表，这个表包含了目标函数的地址。然后一个消息发送会包含调度表的加载地址（被存放到每个对象的第一个字中），索引到这个表来获得目标函数的地址，然后调用这个函数。

调度表在静态类型语言中是容易实现的因为表索引的范围，发送给一个对象的可能的消息集合，是静态已知的，因此这些表的大小（和内容）是容易计算出来的。但是，在动态类型语言中也是可能可以使用调度表的，尽管需要一些加法计算[7,50,135]。这些方法的主要缺陷是它们难以被整合进一个交互系统因为当引入新的消息或者类型时，调度表需要定期地被从新排布；在一个大的系统如 Smalltalk-80，这种从新排布需要花费许多分钟，且调度表会消耗几百KB内存。因此，我们不对调度表做进一步的讨论。

#### 3.3 内联缓存

即使用了查找缓存，发送一条消息仍花费比调用一个简单过程的时间来得长因为许多消息发送都需要先探测缓存。即使在理想场景下，一个缓存查找关联了连续得十条指令来获得接送者类型和消息名，形成缓存索引（例如，通过异或和移位接收者类型和消息名），获得缓存条目，然后比较这个条目和实际的接受者类型和消息名来验证缓存的目标函数是确实是正确的。这个情况下，查找缓存是 100% 有效的（访问都命中），发送仍然相当地慢因为每次发送查找缓存的探测都要加上十条指令。

幸运地是，消息发送可以加速因为看到了在一个给定的调用调用点的接收者类型是很少变化的；如果在一个特定的调用点一个消息被发送给一个类型X的对象，下一次这个发送被执行大概率仍然是X类型的接收者。例如，Smalltalk 代码的几个研究表明，一个给定调用点的接收者类型 95% 的时间里是保持不变的[44，130，132]。

这种类型使用的局部性可以通过在调用点缓存要查找函数的地址来加以利用。因为查找结果被缓存“在线”在每个调用点（例如，在命中场景下不会访问单独的查找缓存），这个技术叫***inline cahcing***[44，132]。图 3-1 显示了空的内联缓存；调用函数简单的包含一个系统查找例程。第一次这个调用被执行，查找例程会找到目标方法。但是在跳到目标前，查找例程改变调用指令去执行刚找到的目标方法（图 3-2 ）。随后发送执行直接跳到目标方法，完全避开任何查找。当然，接收者的类型可以被改变，所以被调用方法的导语必须验证接收者类型是正确的，如果类型测试失败调用查找代码。

{% if 1 == 1 %} 
  {% asset_img figure_3_1.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_1.png)

{% endif %}

{% if 1 == 1 %} 
  {% asset_img figure_3_2.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_2.png)

{% endif %}

因为以下一些原因内联缓存比离线查找缓存快得多。首先，因为每个调用点都有分离的缓存，不需要测试消息名来验证缓存是否命中--只有在未命中的时候测试才必须做（通过系统查找例程）。第二，内联缓存不需要执行任何的加载指令来获取缓存条目；这个功能通过调用指令隐式地执行了。最后，因为没有显示地索引到任何表，我们可以忽略哈希函数的异或和移位指令。唯一的开销是接收者类型的检查通常用非常少的指令完成（在一个典型的 Smalltalk 系统是两个加载和一个比较跳转，在SELF系统是一个加载和一个常量比较）。

内联缓存在降低查找开销上非常的有效因为命中率高且命中开销小。例如，SOAR（一个RISC处理器上的Smalltalk实现）没有内联缓存会慢33% [132]。我们已知的所有的编译实现的Smalltalk都包含内联缓存，包括 SELF 系统。

#### 3.4 处理多态发送

仅当调用点的接收者类型（和调用目标）保持相对固定内联缓存才有效。虽然内联缓存对主要的发送都工作的很好，但是它不能加速有着几个接收者类型的多态调用点因为调用目标会在几个不同的方法来回切换。

例如，假设一个方法是发送 **显示** 消息到一个表中的所有元素，然后表中的第一个元素是长方形：

{% if 1 == 1 %} 
  {% asset_img rectangle.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\rectangle.png)

{% endif %}

如果表中下一个元素是圆形，控制通过内联缓存中的调用即长方行的显示方法。方法导语中的接收者测试检测到类型错误然后调用查找例程重新绑定内联缓存到圆形的显示方法：

{% if 1 == 1 %} 
  {% asset_img figure_3_3.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_3.png)

{% endif %}

不幸地是，如果表中包含的圆形和长方形不是特定的顺序，内联缓存会一次又一次地失败因为接收者类型总是改变。在这个例子中，内联缓存可能比使用离线缓存更慢因为失败次数太多了。特别是，在大部分呢现代处理器中失败代码其中包含改变调用指令需要无效化指令缓存的部分内容。通常，失败处理也包含相当大的额外开销。例如，SELF 系统链接所有的调用特定方法的内联缓存到一张表如此当方法被重定位到不同的地址或者被丢弃它们可以被更新。

内联缓存失败导致的性能影响在高性能系统中变得更严重，以至于不能在被忽视。例如，SELF-90 的评测显示 Richards benchmark 花费 25% 的时间来处理内联缓存失败[23]。

SELF 系统上的一个非正式的多态调用点测试显示大部分的用例多态的度都是比较小的，通常小于十。发送的多态性的度符合三峰分布：大部分的发送是单态的（只有一个接收者类型），部分是多态（少量的接收者类型），非常少的是巨态的（非常多的接收者类型）。图 3-4 显示非空内联缓存的数量分布，例如，调用点展示的多态性的度。这个分布是在使用原型 SELF 用户接口[28]一分钟后获得的。极大部分的调用点都只有一个接收者类型；这是通常的内联缓存也能工作得很好的原因。相当少的调用点有两个不同的接收者类型；一个常见的例子是布尔消息，因为对和错在SELF中是两个类型。极少的调用点有多于五个的接收者类型。（所有的超过十个接收者类型的调用点只有块；节4.3.2 解释了这个巨型态行为。）图 3-4 多态调用的性能可以通过一个更自由的缓存形式来提高因为大部分非单态的发送只有少量的接收者类型。

{% if 1 == 1 %} 
  {% asset_img figure_3_4.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_4.png)

{% endif %}

#### 3.5 多态内联缓存

为了降低内联缓存失败的开销，我们设计和实现了多态内联缓存，一钟内联缓存的新的扩展技术用来高效处理多态调用点。不仅仅是缓存最后的查找结果， 一个多态内联缓存（PIC）为一个给定的多态调用点缓存了几个查找结果到一个专门生成的桩例程里。

在我们发送显示消息到列表元素的例子中，一个系统初始化使用 PICs 就像正常的内联缓存一样：在第一次发送后，内联缓存绑定到长方形显示方法上。

{% if 1 == 1 %} 
  {% asset_img rectangle_2.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\rectangle_2.png)

{% endif %}

但是当遇到第一个圆形时，不是简单地切换调用目标到圆形的显示方法而是失败处理过程会构造一个短的桩例程然后重新绑定调用到这个短的例程上：

{% if 1 == 1 %} 
  {% asset_img figure_3_5.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_5.png)

{% endif %}

桩（类型匹配）检查接收者是长方形还是圆形然后跳转到对应的方法。这个桩可以直接跳转到方法体中（跳过方法导语的类型检查）因为接收者类型已经验证过了。然而，方法仍然需要导语的类型检查因为它们也会被单态调用点调用此时还是标准的内联缓存。

因为 PIC 当前缓存了长方形和圆形，如果列表里只有长方形和圆形就不会再有失败发生。所有的发送都是快速的，在找到目标前只需要一到两个比较。如果缓存失败再次发生（如接收者既不是长方形也不是圆形），桩例程会简单的扩展来处理新的场景。最终，桩会包含实践中的所有场景，不会再有缓存失败和查找。

##### 3.5.1 变化

上面描述的方案在大部分的场景下工作良好将多态发送开销降低到非常少的机器周期。这一节讨论一些固有问题和可能的解决方法。

***Copying with megamorphic sends***。一些发送点可能会发送一个消息到非常大的类型数量。例如，一个方法会发送 writeSnapshot 消息给系统的任意的对象。为这样的发送构建一个巨大的 PIC 浪费时间和空间。因此，内联缓存失败处理不会为超出一定类型数量用例扩展PIC；而是，标记这个调用点为巨态且采用反馈策略，可能只是传统的单态内联机制。

在 SELF 系统中，如果超出一定的大小（当前，10个） PICs 就标记为巨态。当一个巨态 PICs 未命中，它不会为了包含新的类型而增长。而是，其中一个场景会被随机选到然后被替换为新的场景。这个系统的一个早期版本使用 “move-to-front” 策略即在前面插入新的场景，往后移动其它的场景且去掉最后一个场景。但是，这个策略已经被放弃因为它的失败开销太大了（所有的10个条目都需要被改变，而不是只有一个）且它的失败率也有非常高的偶然性，例如类型循环的改变。（是的，墨菲定律是成立的--真实程序反应了这个现象）。

***Improving linear search***。如果每个类型的动态使用频率是已知的，PICs 可以定期重新排序从而将最频繁出现的类型放到 PIC 的开头，降低类型检查执行的平均次数。如果线性搜索不够高效，更复杂的算法如二分搜索或者某些形式的哈希可以被用于许多类型的场景。但是，因为类型的数量平均上是很小的（见图 3-4），这个优化可能不值得大力去做：对于大多数场景一个线性搜索的 PIC 是可能快于其它方法的。

***Improving space efficiency***。多态内联缓存是大于正常的内联缓存的因为桩例程关联了每个多态调用点。如果空间紧张，有着完全相同消息名的调用点是可以共享一个共同的 PIC 从而减少空间固有开销。在这个设想下，PICs 可以充当快速的特定消息查找缓存。一个多态发送的平均开销很可能高于特定调用点 PICs 因为每个 PIC 的类型数量会增加，这个是由于局部性丢失导致的（一个共享 PIC 将会包含特定消息名的所有接收者类型，调用相关的 PIC 只包含实际出现在调用点的类型）。

#### 3.6 实现和结果

我们为 SELF 系统设计和实现了多态内联缓存并且测量它们的效果。这节所有的测量都是在一个有 48 MB内存的轻量加载的 Sun-4/260 上进行的；用于比较的基础系统是1990 年 9 月的 SELF 系统。这个基础系统使用内联缓存；一个直到特定方法的代码的发送需要 8 条指令（9 个周期）。一个内联缓存失败花费 15 毫秒或者 250 周期。失败时间可以通过一些优化和用汇编重编码关键部分来降低。我们估计这些优化可以降低失败的固有开销两倍左右。因此，我们的测量可能会夸大 PICs 的直接性能优势到两倍左右。另一方面，商业的 Smalltalk-80 实现（ParcPlace Smalltalk-80 系统，2.4版本）的测量表明它花费 15 毫秒处理失败，所以我们当前的实现并不是看上去的那样无理由的慢。

我们实验的系统的单态发送使用了和基础系统相同的内联缓存形式。对于多态发送，一个桩被构造来测试接收者类型和跳转到对应的方法。这个桩有个固定的 8 周期开销（加载接收者类型和跳到目标方法），每个类型测试花费 4 周期。PICs 就像 3.5 节描述的那样实现的。前面章节提到的变化部分基本没有实现，除了调用点可能会被当做巨态当它超过十个接收者类型（但是这种调用没有出现在我们的 benchmarks）。

{% if 1 == 1 %} 
  {% asset_img Table_3_1.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\Table_3_1.png)

{% endif %}

表 3-1 总结了查找时间。基础系统里的多态发送开销依赖于多态调用点的内联缓存失败率。例如，如果发送执行每三次更改一次接收者类型，花费将会接近于 250/3 = 83 周期。对于有 PICs 的系统，平均多态查找开销是用例数量的函数；如果所有的 n 个用例都是完全相似的，平均分发将会涉及 n / 2 类型测试所以花费 8 + （n / 2）* 4 = 8 + 2n 周期。

为了评估多态内联缓存的效果，我们测量了一套 SELF 程序。这个程序（有 PolyTest 的异常）可以被认为是相当典型的面向对象的程序能覆盖各种编程风格。更多的 benchmarks 数据在 [70] 中给出。

  **Parser**。一个递归向下的解析器为早期 SELF 语法版本开发的（550行）。

  **PrimitiveMaker**。根据原语描述生成 C++ 和 SELF 桩例程的程序（850行）。

  **UI**。SELF 用户接口原型（3000行）运行一个小的交互界面。因为用于我们的测试的 Sun-4 没有特殊的图形硬件，运行中占主导地位的是图形原语（如多边形填充和全屏位图拷贝）。对于我们的测试，有三个非常昂贵的图形原语被转为无操作；保留的原语仍然占据全部运行时间的 30%。

**PathCache**。SELF 系统的一部分，用于计算所有全局对象的名字然后存储为压缩格式（150行）。大部分时间花在一个循环上其迭代处理一个包含 8 个不同种类对象的集合。

**Richards**。一个操作系统模拟 benchmark （400行）。这个 benchmark 调度执行四个不同的任务。它包含了一个经常执行的多态发送（调度器发送 RunTask 消息到下一个任务）。

**PolyTest**。一个人造的 benchmark（20行）为了显示 PICs 所能达到的最高加速而设计的。PolyTest 由一个 5 度多态发送的循环组成；发送被执行一百万次。正常的内联缓存在这个 benchmark 会有 100% 的失败率（不存在两个递归发送有相同的接收者类型）。因为 PolyTest 是比较小的，人造的 benchmark，所以计算整个 benchmark 集的平均值的时候不会包含它。

##### 3.6.1 执行时间

为了获得更准确的测量，所有的 benchmark 连续运行 10 次然后计算平均的 CPU 时间。这个程序重复10次，选择最小的平均值（假设比较长的时间是因为其它的 UNIX 程序导致的）。一次垃圾回收被执行在每次测量前为了减少不准确性。图 3-6 显示系统使用 PICs 节省的执行时间（详见附录A中的表 A-2 ）。

{% if 1 == 1 %} 
  {% asset_img figure_3_6.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_6.png)

{% endif %}

节省的时间从无（UI）到重大的（Richards，Parser）再到壮观的（PolyTest）；benchmark 节省的时间的中位数是 11% （去掉 PolyTest）。从单个 benchmarks 观察到的加速与基础系统中处理内联缓存失败的时间紧密相关。例如，在基础系统的 PolyTest 花费了超过 80% 的执行时间在失败处理上，然后超过 80% 的执行时间由 PICs 消除。这种密切的相关性表明 PICs 几乎消除了内联缓村失败的所有固有开销。

{% if 1 == 1 %} 
  {% asset_img Table_3_2.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\Table_3_2.png)

{% endif %}

表 3-2 显示了我们的 benchmarks 在基础系统的失败率。虽然 SELF 的实现有自己的优化编译器与 Smalltalk 系统完全不同，但与之前那些 Smalltalk 系统的研究观察到的失败的概率相同的，其观察到的失败概率大约是 5% [44，130，132]。当引入 PICs 时失败率不与加速直接相关因为 benchmark 有完全不同的调用频率（相差五倍以上）。

一个可能的期望是内联缓存失败率相关于程序中存在的多态机制的度，通过多态调用点的消息发送的分数来衡量（如，来自内联缓存即遇到了至少两个不同的接收者类型）。例如，一个假设是一个程序发送的80%的消息来自多态调用点有更高的内联缓存失败率比起一个程序只发送 20% 来自多态调用点的消息。有趣的是，我们的 benchmark 程序中没有这样的场景（图 3-7）。例如，PathCache 有超过 73% 的消息发送来自多态调用点而 Parser 有 24%，但是 PathCache 的内联缓存失败率（5.8%）稍低于 Parser 的失败率（6.2%）。PathCache 中只有一个接收者类型在大部分多态调用点中占主导地位（所以接收者类型基本不改变），而 Parser 的内联缓存的接收者类型频繁地改变。因此，按发送频率对 PICs 类型测试进行排序（如 3.2 节的建议）对于像 PathCache 的程序可能是个胜利。

{% if 1 == 1 %} 
  {% asset_img figure_3_7.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_7.png)

{% endif %}

##### 3.6.2 空间固有开销

PICs 的空间固有开销是比较低的，通常小于编译代码的 2%（见 表 3-3）。低固有开销的主要原因是大部分调用点不需要 PIC 因为它们是单态的。因此，即使一个 2 个元素的 PIC 桩占据超过了 100 字节，整体空间开销仍非常适中。

#### 3.7 总结

传统的内联缓存对于大部分的发送都工作良好。 但是，真的多态调用点（例如，调用点的接收者类型改变的比较频繁）会导致显著的内联缓存开销，我们测量下来要消费 25% 的程序总执行时间。

{% if 1 == 1 %} 
  {% asset_img Table_3_3.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\Table_3_3.png)

{% endif %}

我们用多态内联缓存（PICs）扩展传统的内联缓存从而可以缓存多个查找目标。PICs 非常有效地移除了内联缓存的固有开销，加速了我们的 benchmark的执行达到 25%，中位数是 11%。

虽然我们已经讨论了 PICs 作为加速消息发送的一种方法，但它在SELF系统中的主要目的是为了优化编译器提供类型信息。通过这种类型信息获得的性能提升远远超过本章中观察到的加速。我们将会在第五章详细讨论 PICs 的这方面内容。

### 4. 非内联编译器

前代 SELF 编译器造成的长时间的编译暂停非常分散用户的注意力，威胁要向提升程序员生产力的目标妥协。例如，开始原型用户接口花费了 60 秒，其中 50 秒是编译时间。虽然 SELF 编译器和标准 C 编译器一样快，但仍然很慢。

#### 4.1 简单代码生成

为了提高系统的响应能力，我们决定实现一个非内联的编译器（“NIC”）。它的任务是尽可能快的编译方法，不尝试任何的优化。最后，它的代码生成策略是非常简单的。编译器直接翻译源方法的字节码到机器码中间没有构造中间表达式。

字节码按如下方式转换：

- 源码字字串（如，‘foo’）被加载到寄存器。如果字串是一个块，块克隆原语会第一个被调用来创建块（除了原语失败块，其会被特殊处理，见下面说明）。
- 原语发送（如，_IntAdd:）被转换为一个 C 或者汇编原语相应的调用。
- 发送被转换为动态分发的调用通过生成一个内联的缓存，除了发送访问一个方法的局部变量，此时相关的栈位置被访问。

寄存器分配相当的简单：表达式栈条目（如，表达式已经被评估但还没有被一个发送消费因为其它表达式需要先评估）被分配到一个寄存器中，一个位掩码追踪活跃的寄存器。局部是栈分配的因为它们可能会被内嵌块向上访问；所有的入参（通过寄存器传送）会被压入栈因为相同的原因。这是需要一些代码分析的地方之一（例如，分析一个方法是否包含任何的块）可以加速代码生成。非内联的编译器包含 2600 行 C++。

编译器只执行两个优化，两个都非常的简单易于实现且有已知的显著收益。首先，原语失败块会别延迟创建直到它们需要的时候。例如，整数加法原语 IntAdd：用一个失败块作为参数。当原语失败时块被调用，例如，因为一个溢出。但是原语调用通常时成功的，因此失败块通常不需要。因此，编译器延迟了失败块的创建直到原语实际失败的时候（如，直到它返回一个特殊的错误值）。这个优化加速了程序大概 10-20% 因为它极大地降低了程序的分配率。

第二个优化是访问接收者中的槽被内联，替代一个有加载语句的发送。虽然加速的结果非常的一般（通常只有非常小的百分比），这个优化降低了已编译代码的大小将近 15% 因为它用一个字的加载语句替换了十个字的内联缓存。

此外，NIC 也使用了客制化，虽然没有从中受益太多（接收者实例变量访问的内联是使用客制化的唯一优化）。客制化被使用因为一个假设即所有的代码都是渗透到SELF系统许多部分的客制化（特别是查找系统），所以 NIC 更容易以这种方式被整合进系统。从 NIC 移除客制化可能会有利因为它会减少代码重复。

NIC 不执行一些其它语言的相似编译器通常会执行的优化。尤其，它不以任何方式优化整数计算，没有特例且不内联任何的控制结构（如，ifTrue：）。这些决策的性能影响会在 4.3 节和 4.4 节测试。

#### 4.2 编译速度

NIC 大致使用了 400 条指令来生成一条 SPARC 指令。这个明显慢于 Deutsch 和 Schiffman 描述的简单编译器其每一条生成指令只需要使用 50 条指令[45]。

这个不同有几个原因。首先，SELF 字节码比起 Smalltalk-80 虚拟机的字节码更高级别。例如，SELF 为通常的发送使用相同 Send 字节码，访问实例变量，访问局部变量，和原语调用因为所有这些在 SELF 上是语义完备的。相比之下，Smalltalk-80 有更面向机器字节码格式；事实上，在早期 Smalltalk 实现上字节码直接由微码解释[43]。例如，Smalltalk 有特殊字节码来实例变量访问（特殊槽号码），局部访问，和最常见的原语（算术和控制转换）。因此，Smalltalk-80 字节码编译器（等同于 SELF 解析器）已经执行了一些编译任务如替换简单的控制结构为跳转字节码和替换算术操作为特定的字节码。作为结果，Deutsch-Schiffman 编译器不得不执行少量的转换 Smalltalk-80 字节码到机器码的工作。相比之下，这些信息没有显式的存在于 SELF 字节码中因为它不是为解释或者头脑中的快速编译设计的。

图 4-3 显示一个 NIC 编译后的粗糙的采样数据通过 gprof 采样工具采样大约 8,000 的编译任务获得。有趣地是，实际的编译任务的时间只是编译方法的分配长两倍。不出意外地是，处理消息发送占用实际编译的很大一部分（编译时间的33%，或者总共的 23% ）。但是 Smalltalk 系统中编译器也花费了 17% 的时间（总共的 12%）在解析器的处理任务上，就像槽的搜索，决定一个发送是否是本地槽的访问，和计算哪个字节是语句边界。

{% if 1 == 1 %} 
  {% asset_img figure_4_1.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_4_1.png)

{% endif %}

另一个开销来源是编译器大量地使用抽象数据结构以简化实现。例如，它使用一个代码生成器对象来从前端抽象代码生成细节，所有移植编译器变得简单。类似地，代码生成器使用汇编器对象以生成指令而不是自己操纵指令字。虽然这样汇编器是相当高效的（编译器调用诸如 load(base_reg, offset, destination) 而不是传递一个字符串“load base, offset, dest"），但是这种组织方式阻碍了一些优化如对一些特定常见的指令或指令序列使用预计算模式。另一方面，它简化了系统因为所有的编译器共享相同的汇编器。

另一种描述编译时间的方式是测量每个字节码的编译时间，如每个源码单元。

{% if 1 == 1 %} 
  {% asset_img figure_4_2.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_4_2.png)

{% endif %}

图 4-2 显示了编译时间是相当线性于源方法长度的，虽然相同大小的方法的编译时间会有变化有些时候是因为不同的字节码会花费不同的编译时间。例如，加载一个常量 1 比起编一个发送快了许多：前者直接生成一个机器指令，同时后者生成多达十几条指令首先要检查发送是访问一个局部变量还是接收者中的一个实例变量。平均而言，编译器中每个字节码使用 0.2ms 而一个启动开销是 2 ms；这个线性回归相关性系数是 0.78。大部分编译过程是短的，平均是 3 ms（图 4-3）。

#### 4.3 执行时间

未优化的代码是相当慢的；表 4-1 显示它的性能与优化过的 SELF 程序的关系。在大型程序上，未优化的代码运行起来 9 倍慢于优化 SELF 编译器产生的代码，但是更小的程序如有个小循环可能会更慢。一个极端的例子，BubbleSort 慢了两个数量级。

{% if 1 == 1 %} 
  {% asset_img figure_4_3.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_4_3.png)

{% endif %}

{% if 1 == 1 %} 
  {% asset_img table_4_1.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\table_4_1.png)

{% endif %}

未优化的程序的编译时间都花费在哪了？本节会分析这个问题通过使用 CecilInt （一个巨大的，面向对象的程序）和 Richards （一个较小的，更少多态的程序）为例子来讨论四个主要的原因：寄存器窗口，查找，指令缓存失败，和原语调用 / 垃圾回收。

##### 4.3.1 概述

表 4-2 显示一个简单的 benchmarks 执行时间概况。实际的编译代码约占总执行时间的一半。大约 20-30% 的时间是花费在处理内联缓村失败上的（见 4.3.2 节），另外 20-30% 花费在原语（如整型加法或块分配）和垃圾回收。

{% if 1 == 1 %} 
  {% asset_img table_4_2.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\table_4_2.png)

{% endif %}

表 4-3 显示两个相同程序面向硬件的视图；它包含整个程序执行的数据（即，不仅仅是编译 SELF 代码）。寄存器窗口和指令缓存失败导致了程序的低性能。4.3.4 节和 4.3.5节讨论了两个因素。

{% if 1 == 1 %} 
  {% asset_img table_4_3.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\table_4_3.png)

{% endif %}

##### 4.3.2 查找缓存失败

在不优化的代码中，大部分的调用点是巨型的，即，它们的接收类型在非常大量的接收者类型中变化非常频繁。例如，ifTrue：true 对象的方法发送值到它的第一个参数：

  ifTrue：aBlock = { aBlock value }

ifTrue：发送自许多地方，所有传递不同参数的块。因此，值发送是巨态的因为每个 ifTrue 的使用有不同的接收者类型：在程序中。在当前系统中 PICs 缓存只支持多达 10 个不同的接收者类型。，发送总是在 PICs 中命中失败，造成昂贵的查找失败处理异常。同时，所有的这些失败占据了 Richard 执行时间的三分之一以及 Cecil 的 18%。在优化代码中，ifTrue：会被内联，也允许编译器去内联值发送（因为参数总是一个块字面量）。因此，在优化代码值发送不会导致开销。

##### 4.3.3 块，原语调用，和垃圾回收

NIC 创建更多的块（闭包）超过优化 SELF 代码创建的因为它不优化控制结构涉及块的东西（或者其它的，就此而言）。例如，几个块被创建来给一个循环的每个迭代。同时，块创建占据了两个 benchms 大约 5% 的时间。因为如此多的块对象被创建，所以扫描器（局部垃圾回收）是必须的。幸运地是，这些扫描器非常的快因为大部分对象（基本都是块）是不存活的，所以对于两个程序 GC 开销小于 3% 。不优化的代码调用许多原语，即使是简单的操作像整型算术和比较。同时，所有的原语（包括块分配和 GC）使用大约 20-30% 的总执行时间。

##### 4.3.4 寄存器窗口

SPARC 架构[ 118] 定义了一组重叠的寄存器窗口允许一个进程保存调用者的状态通过切换到一组新的寄存器。只要调用深度不超出可用寄存器组的数量，这样的一个保存可以在一个周期里完成不需要任何的内存访问。如果在一个保存指令执行时没有可用的空余寄存器组，一个“寄存器窗口溢出”陷入会发生然后陷入处理透明地释放一组寄存器通过将它的内容保存到内存中。相似地，一个“窗口下溢”陷入被用于重加载即如果需要再一次的话从内存中刷数据到寄存器组中。不幸地是，未优化的 SELF 代码同时有高的调用密度和高的调用深度；例如，一个 for 循环（通过消息和块实现）调用深度为 13。因此，未优化的代码非常频繁的发生窗口溢出陷入和下溢陷入花费大量的时间处理这些陷入。8.2 节会详细地分析这个开销；当前，可以说在当前的 SPARC 实现上未优化代码的寄存器窗口开销达到了总执行时间的 40%。

##### 4.3.5 指令缓存失败

NIC 的代码生成也会消费许多的空间：每个发送花费 15 个 32 位字在加载参数寄存器指令、调用指令、和内联缓存指令上。此外，每个方法导语大约 15 字（导语会测试接收者映射，增量和未重编译测试调用统计数）。因此，未优化的代码有相当高的指令缓存失败的固有开销；Richards 花费了10% 的时间在等待从内存中加载指令，CecilInt 花费了 27% 的时间在指令缓存失效上。

#### 4.4 NIC 对比 Deutsch-Schiffman Smalltalk

NIC 在很多方面与 Deutsch-Schiffman 编译器比较相似。但是 ParcPlace Smalltalk-80（4.0版本）运行 Richards benchmark 在 3.2 秒，比未优化的 SELF 版本快十倍。为社么 Smalltalk 系统如此的快？有多少的性能差距是语言上的不同导致的，有多少的不同是实现上导致的？

几个不同点可以解释性能差异；通常会想到三个原因，两个是语言不同还有一个是实现不同：

-  硬编码了一组性能关键的方法。就是说，某些消息的含义是固定的所以这些消息的源方法被忽略掉了。通常，Smalltalk硬编码方法实现了 if 和 while 两个控制结构（加上少量的其它循环结构），它也硬编码了整型计算。通过这样做，系统可以定制化（也极大地加速）这些常用的操作而不用实现通用的内联优化。与此相反的，NIC 不定制化任何的操作而总是执行这些消息的发送操作。
-  SELF 程序执行更多的消息发送因为访问实例变量总是通过消息执行，然而 Smalltalk 的方法可以直接访问在接收者中的实例变量而不需要通过发送消息。但是，因为 NIC 内联访问接收者中的实例变量，不应该有许多额外的发送，肯定不足以说明一个数量级的性能差异。
-  NIC 生成的代码可能比 ParcPlace Smalltalk 编译器生成的代码更低效。例如，Smalltalk 编译器在算术消息中使用类型预测然后为整型场景生成内联缓存，然而，NIC 没有类型预测也从不内联原语。此外，NIC 使用 SPARC 的寄存器窗口而 ParcPlace Smalltalk 不使用。但是，定制化整型计算和局部代码的质量不太可能导致将近十倍的性能差异。

##### 4.4.1 不带硬编码控制结构的 Smalltalk 

从这个语言和实现不同的大致测试来说，似乎是第一个--Smalltalk的硬编码控制结构--会对性能产生巨大的影响。为了验证我们的怀疑即 if 和 while 的硬编码是导致性能不同的绝大部分原因，我们禁用了 ParcPlace Smalltalk 系统（4.0版本）中的这些优化，因此所有的控制结构都会生成真实的消息发送。 此外，我们改变了 whileTrue 等的定义类似于它们在SELF 中的等价表示。最后一步是必须的因为这些方法在标准的 Smalltalk 系统中是递归的，所以它们的执行会成为不必要地低效。相反的，它们通过使用新的 primitiveLoop 控制结构来实现即由修改过的编译器来重新组织和开放代码。例如，whileTrue: 的实现如下：

```smalltalk
whileTrue: aBlock
    “repeat aBlock as long as the receiver block yields true”
    [self value ifFalse: [ ^ nil ]. “test condition and exit if false”
     aBlock value.
    ] primitiveLoop
```

这个改变了的 Smalltalk 编译器转换 primitiveLoop 消息为一个简单的向后跳转到方法的开始，就像 _Restart 原语 SELF 用于实现迭代。 在这些改变之后，所有重要的 Smalltalk 控制结构的实现就与它们在 SELF 相同的功能类似，但后者依旧只有少量的优化。例如，whileFalse: 在 SELF 中是通过 whileTrue: 实现的但在 Smalltalk 中是直接通过 primitiveLoop 实现的。因为我们只是要粗略地评估非内联控制结构的性能影响，所以不会不计成本地实现全部的 Smalltalk 控制结构为 SELF 中完全等价的结构。

{% if 1 == 1 %} 
  {% asset_img table_4_4.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\table_4_4.png)

{% endif %}

这些改变对 Smalltalk 的性能产生深远的影响：原始的系统执行 Richards 要 3.2 秒，而改变后的系统需要 33.1 秒。其它的程序也有类似的减慢（Table 4-4）。因此，它表明少量的控制结构的硬链接加速了 Smalltalk 大约 5 到 10 倍！有点意外的是，这个单一的改变缩短了未优化 SELF 代码和 Smalltalk 的大部分性能差距。

##### 4.4.2 一个内联控制结构的 NIC

上述实验出人意料的结果表明 NIC 中相似的优化可以显著地加速它的代码。为了验证这个假设，我们配置优化的 SELF 编译器来生成代码类似于一个假想的 NIC 会内联 if 和 while 控制结构。这个的意图是为了内联 if 和 while 如此控制结构本身就不会涉及任何的块创建或者发送了。例如，表达式

```smalltalk
x < 0 ifTrue: [ doSomething ]
```

被翻译为机器码反汇编为如下序列：

```smalltalk
<send “x”>
<load 0>
<send “<”>
load true, r1
load false, r2
cmp r1, result
beq L1
cmp r2, result
beq L2
<code handling uncommon case>
L1: <send doSomething>
L2: ...
```

对于这个实验，优化编译器的各个部分都被关掉所以生成代码的质量和NIC的代码非常的相似。例如，局部变量没有寄存器分配，没有执行后端代码优化。表 4-5 为 Richards benchmark 比较了标准 NIC 的执行时间和当前的编译器（“NIC+”）；两个版本的 Smalltalk 系统的执行时间都被进行比较了。清楚地看到，内联两个控制结构极大地提高了已编译 SELF 代码的性能，不但因为已编译代码变得更高效（更少的固有开销和块创建）而且因为所有的非内联控制结构造成的查找失败都被消除了。

{% if 1 == 1 %} 
  {% asset_img table_4_5.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\table_4_5.png)

{% endif %}

{% if 1 == 1 %} 
  {% asset_img table_4_6.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\table_4_6.png)

{% endif %}

表 4-6 显示内联 if 和 while 可以平均加速程序到 2.5 倍。一个更好的编译器额外执行优化如寄存器分配和拷贝传播达到 3.1 倍。（我们模拟这个编译器通过使能优化 SELF 编译器中的所有的后端优化同时保留其它的限制。）通过更好的代码质量获得额外的加速是有限的（特别是 Richards），因此一个编译器的真实实现即内联了如 if 和 while 的性能应该和上面的数值比较相似，即使代码质量与我们实验设置的有些不同。

为何 ParcPlace Smalltalk 比起 SELF（Table 4-6） 从内联控制结构（5-10倍，见表 4-4）获得更多收益？有许多的不同导致了这个差异。例如，两组 benchmark 是不同的；同时，基础的 Smalltalk 系统优化了更多的控制结构而不只是 if 和 while。此外，两个系统在许多实现细节上是不同的；例如，NIC 使用客制化而 Smalltalk 不使用。最后，因为 Smalltalk 系统绝不会在没有内联重要控制结构的情况下运行，它可能没有对非内联的场景调整过。因为 Smalltalk 的实现细节不是有效的，我们无法确定确切地差异来源，因此无法得知要多么靠近一个非优化 SELF编译器才可以与非优化的 Smalltalk 编译器。

#### 4.5 一个 SELF 解释器的性能预测

鉴于相对慢速的未优化代码和需要相对大空间开销来存储已编译代码，一个解释器是否可以提供相似的性能而几乎没有空间开销呢？

一个 SELF 解释器面对一个大的挑战：它必须高效地解释消息发送，因为 SELF 中几乎所有的事情都与消息发送相关。两个问题让它变得很难：

- 没有内联缓存，发送是昂贵的。一个直接的解释器不使用内联缓存就不得不使用查找缓存替代。不幸地是，查找极端的频繁：例如，Richards 发送大约 8.5 百万消息（不计算局部变量的访问）。如果我们想要解释器以 NIC 的速度运行允许它 50% 的时间用于查找，一个查找必须花费少于 42*0.5/8,500,000s = 2.5us或者大约 55 条指令（假设 SPARCStation-2 的 CPI 为 1.8，见章节 8）。这已经足够一个查找缓存的探测了，但不清楚是否足够覆盖失败开销。当前系统一个真实消息查找的花费是相当高的，大概在 100 到 1000 us。因此，即使是 1% 的相对较小的未命中率都会导致每个发送分摊 1-10us 的失败开销。
- 字节码编码是非常抽象的。如同上面的讨论，这里只有一个发送字节码，即使是一个局部变量的访问都会被表达为这个字节码。为了高效地解释，它可能需要重新设计字节码格式从“real” 消息发送分离出"trivial"消息发送，或者缓存下面讨论的中间表示。

使用一个重新设计的字节码格式，小心优化的查找缓存，和一个巨大的查找缓存，它大搞可能用解释器达到（虽然不容易）NIC 的速度。通过硬编码少数重要的控制结构，一个解释器可能会超过当前的 NIC。不清楚将会节省多大的空间因为重构方法表示可能会使用更多的空间，且查找缓存将会相当的大因为失败开销非常的昂贵。但是，最有可能的是解释器使用的空间会显著地少于已编译代码。

另一种解释器实现方式承诺以使用更多的空间为代价换取更好的性能。解释器可以动态地翻译源方法到已解释的方法（imethods）然后解释这个 imethods。这种方式的主要优势是 imethod 的格式已经转变为高效的解释形式。

例如，它有内联缓存，会显著地加速发送。同时，空间固有开销会保持得比较低因为只有已经缓存的 imethods 使用了扩展的表示而源方法可以继续的使用更抽象和节省空间的表达。

这种方法的缺点是引入了缓存代码的所有问题和固有开销：从源方法到 imethod 的翻译开销，保持 imethod 代码的开销（分配和取消分配，压缩等），以及持续追踪源改变的开销（当源方法改变，imethod 必须刷新）。根据简单解释器的加速，额外的复杂性和空间开销可能是不合理的。

此外，解释器可以只添加内联缓存到直接解释器通过每个发送字节码添加一个指针来缓存每个发送最后调用的方法；每个方法会缓存最后的接收者类型。Smalltalk 解释器已经使用了相似的组织方式[44]。由于大部分字节码都是发送，这种方式下每个字节码大约需要一个字来缓存。

#### 4.6 NIC 和交互性能

添加一个非优化的编译器到 SELF 系统中的一个目标是降低编译暂停，即提高系统的响应能力。当前的 NIC 在多大程度上实现了这个目标呢？如前一节的讨论，这里有许多编译时间（和简化）和执行时间上的权衡。编译速度加倍好还是执行速度加倍好?

图 4-4 显示了使用 SELF 用户接口进行的一分钟交互的时间线。交互序列被设计来突出初始响应能力：大部分的交互都是第一次执行只有一个“冷”代码缓存，因此必须相当频繁地生成新代码。图 4-5 显示一个大部分代码已经被编译的系统的相似的交互序列。我们通过每秒50次的采样当前系统的活动获得时间线（即，无论是编译还是运行 SELF 代码）然后将结果添加到一个文件中。

基于时间线，我们可以做两个观察：

- 在”冷启动“的时间线中，编译时间超过了 NIC 的执行时间。例如，图 4-4 的前三秒系统花费了相当多的时间在 NIC 中，用来生成上百个新的已编译方法。
- 在两份时间线中，未优化的 SELF 代码只占总执行时间的一小部分且从没有长周期的运行时间。鉴于 NIC 生成的代码非常的不好，所以代码质量对于整体的影响非常的小，让人很意外。

从两份观察可以得到一个更快的编译器或者解释器可以提高系统的响应能力即使它们降低了执行时间。例如，即使两倍慢的速度运行程序解释器也会受益。

{% if 1 == 1 %} 
  {% asset_img figure_4_4.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_4_4.png)

{% endif %}

{% if 1 == 1 %} 
  {% asset_img figure_4_5.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_4_5.png)

{% endif %}

#### 4.7 总结

一个非内联的编译器（NIC）是编译过程的第一阶段。它快速地编译代码（通常每个编译少于 3 ms）但是生成的相当较慢的代码，显著地慢于非优化 ParcPlace Smalltalk 编译器生成的代码。一个使用 ParcPlace Smalltalk 编译器的实验显示在没有硬编码通用的控制结构的情况下，Smalltalk 执行慢了一个数量级。另一个实验显示在 SELF 中一个更复杂的编译器其内联（或硬编码）了 if 和 while 控制结构会有显著地加速（大约2.5倍），代价是加法的实现复杂或者丢失了语言的简单性（如果控制结构被硬编码）。或者，一个 SELF 解释器可能会达到当前 NIC 的速度同时降低内存的使用。

与已知不同的是，未优化的 SELF 代码相对的慢速在实现中并不是问题因为系统不会花费太多的时间在执行未优化代码上。交互的SELF会话的时间线显示比起提高已编译代码的速度，编译时间的提高更有利于系统的响应能力，特别是在“冷启动”场景这里有上百个方法必须被快速编译。

### 5. 类型反馈和自适应重编译

面向对象的程序比起像 C 或者 Fortran 语言写的程序更难以优化。面向对象的编程风格鼓励代码分解和微分编程。因此，程序更小且过程调用更频繁。此外，难以优化这些进程调用因为他们使用动态分发：被调用的过程具体是哪个函数在运行前是未知的因为它取决于接收者的动态类型。因此，一个编译器通常无法在这些调用上应用标准的优化如内联替代或者进程间分析。思考下面的例子（用 pidgin C++ 写的）：

```c++
class Point {
virtual float get_x(); // get x coordinate
virtual float get_y(); // ditto for y
virtual Point distance(Point p); // compute distance between receiver and p
}
```

当编译器遇到表达式 p->get_x()，此时 p 的声明类型是 Point，它不能优化这个调用因为它不知道 p 的确切地运行时类型。例如，Point 有两个子类型，一个是笛卡尔点，另一个是极坐标点：

```c++
class CartesianPoint : Point {
float x, y;
virtual float get_x() { return x; }
// (other methods omitted)
}
class PolarPoint : Point {
float rho, theta;
virtual float get_x() { return rho * cos(theta); }
// (other methods omitted)
}
```

因为 p 在运行时可以是 CartesianPoint 或者是 PolarPoint 对象，编译器的类型信息不够精确不足以优化这个调用：编译器知道 p 的抽象类型（即，即一组可以调用的操作，和他们的签名）但不是确定的类型（即，对象的大小，格式，和操作的实现）。

重要的是要意识到不是动态类型导致的动态分发调用的问题：如我们上面看到的例子，它也会发生在静态类型的语言上。而是，封装和多态导致的问题，即隐藏了来自对象客户端的对象实现细节和提供了相同抽象类型的多种实现。如果只能访问到程序的源码，静态语言的编译器除了为通用场景的消息发送生成一个动态分发的调用没有更好的方案了。静态类型使得编译器可以检查 get_x() 的消息是合法的（即，对接收者 p 保证有一些实现一定存在），但不能识别哪个实现会被调用。因此，对于消除动态分发调用的问题，我们研究一个动态语言如 SELF 或者研究一个静态类型的语言如 C++ 都是相差不大的。

纯的面向对象的语言加剧了这个问题因为每个操作都调用一个动态分发的消息发送。例如，即使是非常简单的操作如对象变量访问，整型加法，或者数组访问理论上在 SELF 中都调用消息发送。因此，一个纯的面向对象的语言如 SELF 为处理频繁动态分发调用问题的优化技术提供了一个理想的测试集。

我们已经开发了一个简单的优化技术，类型反馈，从运行时系统反馈类型信息到编译器。有了反馈，编译器可以内联任意的动态分发的调用。我们已经为 SELF 系统实现了类型反馈且与自适应重编译结合在一起：已编译代码最初以未优化的形式创建来节省编译时间，时间敏感的代码在后面被重新编译和优化通过使用类型反馈和其它优化技术。虽然我们只为 SELF 实现类型反馈，但这个技术是语言独立的可以应用于静态类型，非纯语言等上面。本章的剩余部分会描述当前工作的基本思想不会涉及到太多的 SELF 相关的细节。下一章会讨论基于此处描述的原理实现的新 SELF 系统的细节。

#### 5.1 类型反馈

面向对象语言（或者其它支持某些延迟绑定的语言，如 APL）主要的实现问题是编译时缺乏静态可用的信息导致的。这是因为，一些操作的确切含义无法静态地确定依赖于动态（即，运行时）信息。因此，只是基于程序文本，非常难以静态地优化这些延迟绑定的操作。

这边有两个方式来解决这个问题。第一个是，动态编译（也叫延迟编译），将编译放到运行时这时有额外的信息可以更好地优化延迟绑定操作。SELF 和之前其它的几个系统（如，van Dyke APL 编译器[51]）采用这个方式。与这些系统比起来，SELF 多延迟了一步不尝试立即生成最好的代码（即，当编译代码是第一次需要）。而是，系统首先生成不优化的代码只有之后生成时在生成优化的，即在清楚代码经常使用之后。除了明显节省编译时间，这种方式生成的代码可能比 “eager” 系统生成的代码更好因为编译器有更多可用的信息。

如果不能（或不想）把编译放到运行时，可以使用第二种方式，更常规的方式即将额外的运行时信息放到编译器中。典型地，这些信息通过独立的运行收集然后写入文件，编译器被重新调用然后使用这些额外的信息生成最后的优化程序。

这些方式中都有类型反馈。现在，我们会集中于第一个方式因为它是 SELF 使用的；第二个方式会在5.6节中简单概述。

类型反馈的关键点是从运行时系统扩展类型信息然后反馈到编译器。具体地说，一个带有检测的程序版本记录程序的类型配置信息，即，程序中每个单一调用点的一系列接收者类型（还可能有它们出现的频率）。为了获得类型配置信息，标准的方法分发机制不得不用一些方式进行扩展来记录想要的信息，如，每个调用点保存一个接收者类型表。

当获得程序的类型配置信息，这些信息随即被反馈给编译器所以它可以优化动态分发调用（如果愿意）通过预测比较可能的接收者类型然后这些类型的调用。在我们的例子中，类型反馈信息会为 get_x() 调用预测为 CartesianPoint 和 PolarPoint 接收者，然后表达式 x = p->get_x() 可以被编译为：

```smalltalk
if (p->class == CartesianPoint) {
// inline CartesianPoint case
x = p->x;
} else {
// don’t inline polar point case because method is too big; this branch also covers all other receiver types
x = p->get_x(); // dynamically-dispatched call
}
```

对于 CartesianPoint 接收者， 上面的代码序列会执行的非常的块因为它去掉了原始的虚函数调用转变成了一个比较和一个简单的加载指令。

类型反馈实际上改变了我们对面向对象语言的优化问题的理解。在传统的视角中，实现级别的类型信息是稀缺的，因为程序短通常没有足够的信息来分辨 p 是 CartesianPoint 还是 PolarPoint。新的观点来自于意识到尽管最初可供编译器使用的实现类型信息确实很少，但却是因为在程序运行一段时间后运行时系统的信息被抛弃了。在 SELF 中，每个内联缓存或者 PIC 包含一个该发送所遇到的确定的接收者类型列表。换句话说，不需要额外的模块提供给类型反馈如果系统使用了多态内联缓存。因此，一个程序类型配置信息是一应俱全的：编译器只是需要去检查程序的内联缓存来知道一个调用点到目前为止哪些接收者类型已经被遇到了。在我们的例子中，对于 get_x 发送的内联缓存将会包含 Cartesian 和 PolarPoint 类型。如果编译器已经访问了这些类型信息，它可以内联任何发送因为所有的接收者是已知的。

使用类型反馈提供的类型信息会实际上简化编译。当缺少类型反馈，SELF-91 编译器执行大量的类型分析试图保留和传播稀缺的类型信息给编译器。使用类型反馈，主要的问题不在是怎样内联一个动态分发调用而是如何选择一个调用来内联。从执行类型分析的艰巨负担中释放出来，面向对象的语言的优化编译器应该是容易编写的不会牺牲代码质量。新的 SELF-93 编译器基于类型反馈只有之前编译器的一半大小，然而它可以内联更多调用，从而生成更快的代码。

#### 5.2 自适应重编译

SELF 系统使用自适应重编译不只是为了利用类型反馈，也是为了界定应用的哪个部分应该被完全优化。下面的图片显示了 SELF-93 系统整个编译过程的概况：

 {% if 1 == 1 %} 
  {% asset_img figure_5_1.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_5_1.png)

{% endif %}

当一个源方法第一次被调用，它被一个简单的，完全非优化的编译器（第四章）编译为了非常快速地生成代码。有一个非常快速的编译器来降低编译暂停对于一个使用动态编译的交互系统是必要的，正如我们将在第 9 章中看到的那样。如果这个方法经常被执行，会使用类型反馈对其进行重新编译和优化。有些时候，优化过的方法被重新编译为了使用额外的类型信息或者使其适应程序类型配置信息的变化。

SELF 系统必须在没有程序员干预的情况下发现重新编译的机会。通常，系统必须决定

- 什么时候重新编译（等待类型信息积累多长时间），
- 什么被重新编译（哪些编译后的代码最能从附加类型信息中受益），以及
- 哪个发送内联，哪个发送分离作为动态分发调用。

下面的章节会讨论这些问题。这里介绍的解决方案都采用了简单的启发式方法，但是尽管如此，我们稍后会看到效果很好。

#### 5.3 什么时候重编译

SELF 系统使用计数器来识别重编译候选者。每个未优化方法有自己的计数器。在这种方法的导语部分，该方法递增计数器并将其与限制进行比较。如果计数器超过了限制，重编译系统被调用来决定哪个（或者全部）方法应该被重编译。如果方法的计数器溢出了但没有重新编译，计数器会被重置为 0。

如果什么都没做，每个方法可能最终都会到达调用极限然后将会被重新编译即使它的执行频率可能不会超过每秒几次，所以优化将难以带来任何益处。因此，调用计数器呈指数衰减。衰减率作为半衰期时间，即，计数器降低一半值的时间。衰减过程通过周期性地将计数器除以常数 p 来近似；例如，调整计数器的进程每4秒唤醒一次则半衰期就是15秒，常数因子是 $p = 1.2$ （因为 $1.2^15/14=2$）。衰减进程将来计数器从调用次数转为调用频率：给定调用极限 N 和衰减因数 P，一个方法在每个衰减周期里必须执行超过 $N*(1-1/p)$ 次才会被重新编译。

原始地，计数器总是预想的第一方式，被使用直到一个更好的解决方法被找到。在我们的实验过程中，我们发现触发机制（”when“）对于良好的重新编译结果而言，远不如选择机制重要（”what“）。因为简单的基于计数器的方法效果良好，我们没有在大量地考察其它机制。但是，有一些关于调用计数器衰减的有趣问题：

- 指数衰减是正确的模型吗？理想地，系统会重新编译那些优化开销远小于未来被调用累积的收益的方法。当然，系统不知道方法未来被执行的频次，但是基于频率的测量也忽略了过去：执行频率低于最低执行率的方法将永远不会被触发重编译，即使它被执行了无数次。
-  调用极限 N 不应该是个常数；而是应该取决于一个特定的方法。计数器真正想测量的是运行非优化代码浪费的时间。因此，一个会从优化中受益的方法应该计数的更快（或者有个更低的限制） 比起一个不会从优化中受益的方法。当然，可能很难估计优化对特定方法的性能影响。
- 当执行在一个更快速（或更慢）的机器上半衰期要怎么适配？假设原始的半衰期参数是 10 秒，但是系统运行在一个新的机器上有两倍的快。半衰期参数应该被改变吗，如果要，怎么变？一种观点是将更快的机器视为一个实时运行减半快的系统（因为每秒完成的操作是之前的两倍），因此降低半衰期为 5 秒。但是，人们也会质疑调用频率极限是绝对的；如果一个方法执行少于 n 次每秒，它不值得被优化。
- 相似地，应该用实时的，或是 CPU 时间，还是一些机器相关单元来计算半衰期时间（如，执行的指令数）。直观上，使用实时的似乎是错误的，因为使用者的思考暂停（或者咖啡暂停）也会影响重编译。使用 CPU 时间也有它的问题：例如，大部分时间被花费在虚拟机上（如，垃圾回收，编译，或者图形原语），半衰期被有效地缩短因为编译方法获得较少的时间来执行和增加它们的调用计数器。另一方面，这种效果可能是可取的：如果没有更多的时间被花费在已编译的SELF代码中，优化的代码不会增加太多的性能（当然有例外，如果优化降低了虚拟机的固有开销，如，通过降低块闭包的创建数量，然后降低分配开销和垃圾回收）。

虽然这些问题引发了许多有趣的讨论，此处不再赘述，大部分因为简单的方案就工作良好了还有就是时间的限制。但是，我们相信它们是未来研究的有趣方向，调查它们最终可能会有更好的重编译决定。

#### 5.4 重编译什么

当一个计数器溢出，重编译系统被调用来决定哪个方法被重新编译（如果任意）。一个简单的策略是总重新编译那些计数器溢出的方法，因为它显然总是会被重新调用。但是，这个策略不总是有效的。例如，假设方法溢出它的计数器只是返回一个常量。优化这个方法将不会受益许多；但是这个方法应该被内联进它的调用者。因此，通常，为了找到一个好的重编译候选，我们需要遍历调用链然后检查触发重编译的方法的调用者。

##### 5.4.1 重编译过程的概述

图 5-2 显示一个重编译过程的概况。从方法的计数器溢出开始，重编译系统向上走栈寻找一个”好“的重编译候选（下一节会解释”好”的含义）。当一个重编译被找到，编译器被调用来重编译方法，然后老的版本被忽略掉。（如果没有重编译者被找到，执行正常继续。）在优化编译过程中，编译器会标记重启点（即，执行会被唤醒的点）然后计算这个点所有活着的寄存器。如果这个编译成功的话，重新优化的方法会替换栈上相对应的未优化的方法，可能用单个优化的激活记录替换多个未优化的激活记录。然后，如果新的优化方法不在栈顶，重编译继续在新优化过的方法的被调用者上进行。在这个方式中，系统优化整个调用链从顶部重编译者到当前执行点。（通常，重编译调用链只是一个或者两个已编译方法深度。）

 {% if 1 == 1 %} 
  {% asset_img figure_5_2.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_5_2.png)

{% endif %}

如果未优化的方法不能在栈上被替代，它们会被留下来完成它们当前的激活，但是随后的调用将会使用新的，优化过的方法。替换未优化方法失败的主要影响是会发生额外的重编译如果未优化的方法继续执行一段时间。例如，优化过的方法包含一个循环但无法在栈上被立即替换，重编译系统可能随后尝试只替换优化过的代码的循环体。

##### 5.4.2  选择方法进行重编译

SELF-93 系统通过检测几个指标来选择重编译的方法。对于任何的重编译方法 m，下面的几个值被定义：

- m.size 是 m 的指令数量。
- m.count 是 m 被调用的次数。
- m.sends 是 m 直接产生的调用的数量。
- m.version  记录了 m 已经被重编译了多少次。

搜索一个重编译者可以概括为如下。让 trip 作为刚触发了计数器的方法，recompilee 作为当前的重编译候选。

1. 以 recompilee = trip 作为开始。
2. 如果 recompilee 有闭包参数，选择闭包的词法封闭方法如果它满足上面描述的条件。这个规则消除闭包通过内联闭包的使用到闭包里。如果内联成功，闭包通常可以完全优化掉。
3. 否则，选择重编译者的调用者如果它满足下面的条件。这个规则会向上走栈直到遇到一个方法它或者太大了或者不会出现导致许多消息发送被执行。
4. 重复步骤 2 和 3 直到 recompilee 不在改变。

无论何时重编译系统考虑一个新的编译者 m （在上面的 2 和 3 里），它将只会接收新的重编译者如果它遇到了下面的两个条件：

- m.count > MinInvocations 和 m.version < MaxVersion。第一个条款保证方法已经被执行足够多了次数可以考虑它的类型信息的代表。第二个条款防止无休止的重编译同一个方法。
- m.sends > MinSends 或者 m.size <  TinySizeLimit 或者 m 未优化。第一个条款接受发送许多消息的方法，剩下的两个接受那种很可能已经通过内联与被调用者合并了的方法。

这些规则基于的假设是频繁执行的方法值得被优化，内联小的方法和消除闭包会导致更快地执行。虽然规则是简单的，但他们似乎可以很好地找到应用程序的“热点”，如第 7 章所示。（探索更准确的方式来评估潜在的保存和评估重编译的开销仍然是未来工作的一个有趣领域。当然，在系统的动态决策过程中（在运行时），做更多精确评估的额外开销也不得不考虑到。）

重编译系统用来找寻一个“好”重编译候选的规则在许多方面镜像了编译器用来选择“好”内联机会的规则。例如，跳过“微小”方法的规则在编译器中有个等价的规则可以使得“微小”方法被内联。理想地，重编译系统在每次决定向上走栈之前要咨询编译器来确保编译器会内联发送。但是，这样的系统是不切实际的：为了做出内联决定，编译器需要太多的上下文，诸如调用者和其它候选者结合时的全部大小（见 51页的 6.1.2 节）。因此，在这样的系统里重编译决议是昂贵的，故这种方式被拒绝了。但是重编译系统和编译器确实共享着 SELF-93 系统里的通用结构；本质上，重编译器走栈的标准是编译器内联标准的子集。

在重编译之后，系统也会检测重编译是否有效，即，它是否实际上提高了代码。如果先前的和新的已编译方法具有完全相同的非内联调用，重编译不会有真正的收益，因此新的方法被标记在将来的重编译不会被考虑了。

##### 5.5 什么时候使用类型反馈信息

当编译一个特定的消息发送，定位一个相关的类型反馈信息是相对简单的。确定这些信息的可信程度非常困难因为一个方法的类型信息可能合并了几个调用者的类型。例如，一个调用者可能使用了一个有极坐标参数的方法 m 而另一个调用者使用了笛卡尔坐标点。在这个用例中，发送到该参数的类型反馈信息将包含两种类型。如果我们在内联 m 到第一个调用时用到这些信息，编译器会在 m 中专门为了这两种类型特定化一个发送即使只有极坐标点曾经发送过，且会浪费代码空间和编译时间。因此，如果一个已编译的方法有超过 N 个调用者，当内联缓存包含超过 M 个类型时（N， M > 0），类型信息会被我们的编译器忽略。伴随着一个巨大的 N 和 M，代码大小和编译时间会增加因为预测的接受者类型集变得很大；而 N 和 M 比较小时，类型反馈的效果下降了因为更多的接收者类型变成未知了由于类型反馈信息被认为是“被污染了”。我们的实现当前使用了 N = 3 且 M = 2 试图在极端中找到平衡。幸运地是，大部分内联缓存只包含一个类型（M= 1），所以它们的信息可以被信任而不用管调用者的数量。

当然，即使有这些预防措施，基于过去的接收者类型预测未来的接受者类型仅仅是一个比较有根据的猜测而已。优化编译器也会做相似的猜测取决于从前一次运行获得的执行画像[137]。但是似乎一个程序的类型画像比起时间画像更稳定[ 57]--通常，在运行时不会创建新的类型了。因此，当一个程序已经运行了一段时间，新的类型不太可能出现了，除非发生异常情况（例如，错误）或程序员更改应用程序。

##### 5.6 添加类型反馈到一个传统的系统

类型反馈不需要SELF-93中使用的“特殊的”实现技术（如，动态编译或者自适应重编译）。如果有的话，这些技术让它更难以优化程序：在一个交互系统中使用动态编译对编译速度和空间效率有很高的要求。因为这些原因，SELF-93 的类型反馈实现不得不处理不完整的信息（即，部分类型画像和不精确的调用计数）以及要避免一些优化从而可以达到好的编译速度。

# 总结


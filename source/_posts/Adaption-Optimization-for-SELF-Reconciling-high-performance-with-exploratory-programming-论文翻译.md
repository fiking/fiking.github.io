---
title: >-
  Adaption-Optimization-for-SELF:Reconciling-high-performance-with-exploratory-programming
  论文翻译
tags:
  - virtual machine
categories:
  - dissertation translation
  - virtual machine
date: 2021-08-28 22:02:27
description:
---

# 前言

  为了增加虚拟机方面的理论知识，开始了虚拟机相关的论文翻译和学习。先制定个伟大的目标，前期每周一篇翻译，后续在根据学习情况进行论文的总结精炼。

  开篇文章为 **Urs Hölzle** 1994年的博士论文《Adaption Optimization for SELF: Reconciling high performance with exploratory programming》。根据网上资料此文章是 OpenJDK、V8的主要思想来源，所以优先进行阅读。

 <!-- more -->

# 翻译

## 论文题目

SELF 自适应优化：探索性编程与高性能共存

## 作者

Urs Hölzle

## 时间

1994年八月

## 正文

### 摘要

面向对象语言带来许多的益处，其中的抽象性，可以让程序员隐藏来自对象客户端的对象实现细节。不幸的是，在跨越抽象边界如频繁的过程调用中总是伴随着巨大的运行时开销。因此，尽管频繁的使用抽象是这类语言的设计目的，但这是不切实际的，因为，它会导致程序非常低效。

激进的编译器优化会降低抽象的开销。但是优化编译器带来长的编译时间会延长编程环境反馈程序变化的时间。此外，优化也会与源码级别的调试冲突。因此，程序员面临一个两难的选择，不得不在高效的抽象和高效的编程环境之间进行选择。本论文会展示如何通过延迟编译调和这个看起来是矛盾的目标。

融合四个新的技术来达成高性能和高响应：

- ***Type feedback***  通过允许编译器基于获取自运行时系统的信息内联消息发送（message send）来取得高性能。平均来说，在有类型反馈的SELF系统上，程序运行会比在之前的 SELF 系统快1.5倍；而与商业的 Smalltalk 实现对比，两个中型的 benchmark 会快三倍。这个水平的性能是从一个比之前 SELF 编译器更简单和更快速的编译器上取得的。
- ***Adaptive optimization*** 在取得高响应的同时不会牺牲性能，通过使用非优化编译器生成初始代码，同时在运行期间使用优化编译器重编程序中重度使用的代码部分。在前一代的工作平台上如 SPARCstation-2，50 分钟的交互中有将近200个暂停超过200ms，21个暂停超过了 1 秒。在当代的工作平台上，只有13个暂停超过400ms。
- ***Dynamic deoptimization*** 在需要时通过隐式地重建非优化代码，使得程序员可以远离调试优化过的代码的复杂性。不管一个程序是否被优化过，它都可以被停止，检查和单步调试。比起之前的方法，去优化（deoptimization）支持更多的调试的同时，对于可执行的优化代码添加更少的限制。
- ***Polymorphic inline caching*** 实时生成类型信息序列来加速来自于相同调用位置但具有不同的类型对象的消息发送。更重要的是，它会为优化编译器搜集了具体的类型信息。

具有高性能和良好的交互行为的这些技术，让探索性编程既适用于纯粹面向对象语言，也适用于需要更高最终性能的应用领域；调和了探索性编程，繁多的抽象和高性能。

### 致谢

作者的感谢，暂不翻译。

### 目录列表

暂不翻译。

### 图片列表

暂不翻译。

### 表列表

暂不翻译。

### 1. 引言

面向对象编程越来越流行，因为它让编程更简单了。它允许程序员隐藏来自对象客户端的实现细节，将每个对象放入对应的抽象数据类型中，操作和状态都只能通过消息发送进行访问。延迟绑定极大地提高了抽象数据类型的能力，通过允许相同抽象数据类型的不同实现，在运行时可以实现进行互换。这个互换指的是一段调用某个对象的一个操作的代码并不总是最终会被执行调用的代码：延迟绑定（也叫动态分发）会选择一个操作合适的实现基于对象的准确类型。因为延迟绑定对于面向对象是最基本的，需要高效的实现来支持。

理想状态下，面向对象语言会全部使用延迟绑定的，即使对于实例变量访问这种基础的操作也是一样。越普遍地使用封装和动态分发，最终的代码具有越高的自由度和重用性。但是延迟绑定会带来效率问题。例如，如果所有的实例变量方位都使用消息发送，编译器无法将一个对象的属性x的访问翻译为一个简单的加载指令，因为对象间的x实现可能是不同的。例如，一个笛卡尔点可能只是返回一个实例变量的值，而一个极坐标点会根据半径和角坐标计算x的值然后进行返回。这种变化恰恰是延迟绑定的意义：x的操作与操作的实现（实例变量的访问和计算）的绑定延迟到了运行的时候才做。因此，x的操作必须编译成一个动态分发的调用（也叫间接函数调用或者虚函数调用），从而可以在运行时选择一个合适的实现。这导致了一个周期的指令变成了十个周期的调用。随着源码自由度和重用度的增加会带来显著的运行时开销；从而看起来封装和高效是不能共存的。本文会展示如何平衡两个。

一个相似的效率问题也出现在渴望使用探索性编程环境上。一个探索性编程环境增加了程序员的生产率通过对所有的编程操作给与立即的反馈；零暂停交互允许程序员集中于手上的任务而不被长时间的编译暂停分心。通常来说，系统设计者在探索性编程环境会使用解释器和非优化的编译器。不幸的是，解释器的开销，伴随着动态分发的效率问题，降低了执行性能，因此限制了这种系统的使用。这篇论文描述如何降低动态分发的开销从而保证一个交互探索编程环境的反馈获取。

我们研究的轮子是面向对象语言 SELF。SELF 的纯语义加剧了面向对象语言面临的实现问题：在 SELF，每个单操作（甚至赋值操作）都涉及延迟绑定。因此，这个语言是降低延迟绑定优化的理想测试用例。同样重要的是，SELF是为了探索编程设计的。每个探索编程环境必须提供程序变化之后快速的转化，和局部程序的简单调试，从而增加程序员的生成率。因此，一个这种系统的优化编译器不仅要解决动态分发带来的性能问题，也要适配交互性：编译必须快速和非侵入性的，还有系统必须支持全时间段的全源码调试。

我们为SELF实现了一个系统，工作的贡献有：

- 一个新的优化策略， ***Type Feedback***，允许任意的动态分发调用被内联。在我们为 SELF 做的实现样例中，类型反馈将函数调用降低了四倍同时提高了 70% 的性能比起没有类型反馈的系统。尽管 SELF 和 C++ 是完全不同的语言模型，新的系统使得 SELF 在两个中型的面向对象的程序达到了优化过的C++性能的一半。
- 一个重编译系统，动态重新编译应用的 "hot spots"。这个系统通过一个快速的非优化编译器产生初始化代码，然后通过一个慢的优化编译器重新编译那些时间紧迫的部分代码。引入自适应动态编译戏剧性地提高了 SELF 系统的交互性能，让优化编译和探索性编程环境结合成为了可能。即使在前一代工作平台如 SPARCstation-2，50 分钟的交互中有将近200个暂停超过200ms，21个暂停超过了 1 秒。
- 一个内联缓存的扩展， ***polymorphic inline caching***，可以加速多态   调用点的动态分配调用。此外多态内联缓存可以做为类型反馈的类型信息源，平均提升11%的性能。
- 一个调试系统，动态退优化为全局优化代码提供源码级别的调试。即使优化代码只支持严格的调试，这个系统可以隐藏那些限制，提供全源码级别的调试（例如，单步调试这类调试操作）。

虽然我们的实现基于动态编译，但是本文中大部分的技术不需要它。类型反馈会直接被整合进传统的编译系统（看5.6节），类似于其它的基于 profile 的系统。源码级别的调试可以通过保留编译前的未优化代码到一个分离的文件来实现（看10.5.3）。多态内联只需要一个简单的桩生成器，并不需要全成熟的动态编译。只有5.3节描述的动态编译系统--从本质上而言--才需要动态编译。

本文描述的技术都不仅仅限定于 SELF 语言。就如10.5.3节的讨论，调试系统具有非常大的语言无关性。类型反馈可以优化任何语言的延迟绑定；如和面向对象语言相比，那些有重度使用延迟绑定的非面向对象语言（如，APL的通用操作符，Lisp的通用算术）也会从这种优化中获得收益。最后，任何使用动态编译得系统都可能从自适应编译中获取提高性能和降低编译暂停得收益。

本论文剩余部分，我们会描述这些技术得设计和实现，并且评估它们对于 SELF 系统的性能影响。所有的技术都全部实现并且足够稳定地成为了公共 SELF 发布包的一部分。第二章呈现 SELF 的概述和随后章节描述的工作概述，以及讨论相关工作。第三章讨论如何动态分发可以被运行时系统优化，例如，编译器外。第四章描述非优化的 SELF 编译器和评估它的性能。第五章描述类型反馈如何降低动态分发的开销，第六章描述 SELF 优化编译器的实现。第七章和第八章评估新的 SELF 编译器对于之前SELF系统的性能，对其它语言的性能，以及调查硬件特性对性能的影响。第九章讨论优化编译器怎么对系统交互行为产生影响。第十章描述我们的系统怎样提供全源码级的调试且不影响编译器的优化性能。

如果你非常的匆忙且已经很熟悉之前的 Smalltalk 或者 SELF 的实现，我们建议你可以跳过第二章（简介），然后阅读每一章末尾的总结，加上结论。

143页的词汇表包含了本文大部分重要术语的简短定义。

### 2. 背景和相关工作

这个章节展现本论文的一些背景和相关工作。首先，我们简短的介绍 SELF 语言和它的目标，其次是 SELF 系统的概述。然后，我们的新系统的编译过程，最后我们会回顾相关的工作。

#### 2.1 SELF 语言

SELF 是一个 动态类型，基于原型和面向对象的语言，最初的设计是1986年 David Ungar 和  Randall B. Smith 在 Xerox PARC 做的。想要作为 Smalltalk-80 编译语言的一个备选，SELF尝试最大化程序员在探索性编程环境的生产力通过保持语言的简单性和纯净，而不降低表达能力和扩展性。

SELF 是一种纯粹的面向对象语言：所有的数据都是对象，且所有计算的执行都是通过动态绑定消息发送的（包括所有的实例变量访问，即使在接收者对象里）。因此，SELF合并状态和行为i：语法上，方法调用和变量访问是不区分的--发送者不知道消息是简单数据访问的实现还是方法的实现。于是，所有的代码时表达独立的，因为相同的代码可以被不同结构的对象重复使用，只要这些对象正确实现期望的消息协议。换句话说，SELF支持全抽象数据类型：只有对象的接口是可见的，所有的实现细节如对象的大小和结构都被隐藏，即使代码在对象本身里。

SELF 其它主要的重点如下列表：

- SELF 是动态类型的：程序包含无类型的定义。
- SELF 是基于原型的而不是类。每个对象是自描述的可以被独立改变。除了这种方法的灵活性，基于原型的系统可以避免元类（metaclasses）带来的复杂度。
- SELF 有多继承。多继承设计在这些年经历了一些改变。SELF-87 只有单继承，但是 SELF-90 引入了 优先多继承结合一个新的隐私机制为了更好的封装和一个“发送者路径判定”规则为了消除相同优先级父节点的歧义。最近，钟摆又摆回了简单性：SELF-92 消除了发送者路径判定因为它倾向于隐藏歧义，而 SELF-93 消除了优先级继承和来自语言的隐私。
- 所有的控制结构式用户定义的。例如，SELF没有 **if** 语句。作为替代，控制结构通过消息发送和块（闭包）实现，就像 Smalltalk-80。但是，不同于 Smalltalk-80的是，SELF的实现没有固化任何的控制结构，换句话说，程序员可以改变任何方法（包括那些实现if语句，循环和整型加法的方法）的定义，系统会忠实地反射这些变化。
- 所有的对象是堆分配的且被垃圾回收器自动解除分配。

这些特性被设计来发挥现代计算机的计算能力，使得程序员的生活更简单。例如，表达独立性使得重复使用和重组代码变得简单，但是产生了实现问题，因为每个数据访问都关联了一个消息发送。比起关注最小程序窒执行时间，SELF更关注于最小编程时间。

#### 2.2 SELF系统

SELF系统的实现目标反映了语言：最大化编程生产效率的特点。几个特性为这个目标做出贡献：

- ***Source-level  sematics***。系统的行为总是可以用源语言级别的术语解释。程序员应该从不面对一些如 “segmentation fault"，”arthmetic exception: denormalized float“ 的错误，因为这些错误在缺少对底层实现细节的深入了解是无法解释的，这些实现细节超出了语言的定义因此难以理解。因此，所有的 SELF 原语都是安全的：算术操作测试溢出，数组访问执行下标边界检查等。
- ***Direct execution sematics(interpreter semantics)***。 系统应该总是表现如同直接执行方法源码：任何源码改变都是立即生效的。直接执行语义使得程序员免于担心不得不显式调用编译器和链接器，免于不得不处理编译依赖等枯燥的细节。
- ***Fast turnaround time***。 在做出变化之后，程序员不会因为慢速编译器和链接器而等待；这类的延迟应该尽可以能得短（理想下，只是几分之一秒）。
- ***Efficiency***。最后，程序应该总是有效率的运行不受 SELF纯度的影响：程序员不应该因为选择 SELF 而不选择更传统的语言而受惩罚。

这些目标不是轻易能达成的。特别是，因为强调成为程序员正确的语言而不是成为机器的语言，SELF 是很难实现的有效率的。几个关键的语言特性创造了特别难的问题：

-  由于所有的计算都是由消息发送执行的，在底层实现中调用频率极其高。如果简单和通用的计算，这些在传统的编译器中通常只需要单条指令（如一个实例变量访问或者一个整形加法），都调用动态分发的过程调用，程序将会比传统语言运行慢上许多，即使没有一百的慢。
- 相似的，由于没有内置的控制结构，一个简单的 **for** 循环涉及数十条消息发送和几个块（闭包）的创建。因为控制结构的定义可以被用户改变，编译器不能走捷径地将它们的翻译和手写优化代码模式硬关联在一起。
- 因为总体目标是最大化程序员生产率，系统应该有高频的交互和提供立即的反馈给用户。因此，长的编译暂停（出现在优化编译器中的）是无法接受的，进一步限制了实现者在找寻有效的 SELF 实现的自由。

下面的章节给出当前SELF实现的概述和它是怎样尝试解决上面描述的问题的。在描述基础系统之后，我们会强调新的解决方案，这是这篇文章的主题，以及它们怎么适配已经存在的系统。

##### 2.2.1 实现概述

 SELF 虚拟机有如下几个子系统组成：

- 一个 ***memory system*** 处理分配和垃圾回收。对象被存放在堆中；一个分代的扫描垃圾回收器回收不使用的对象。对象引用通过两个比特位标签标记在每个32位字的低两位上（标签00 是整形，01 是指针，10 是浮点，11是对象头）。所有的对象处理整形和浮点由至少两个字组成：一个字表示header，一个指针指向对象 map。一个 map 描述对象的格式，可以被看成是对象低级别类型。有相同格式的对象共享相同的map，所以对象的布局信息只会被存储一次。为保持语言规定的自描述对象的抽象，map是写时拷贝：l例如，当一个槽被添加到一个对象时，这个对象会得到一个新的map。
- 一个 ***parser***  读取文本描述的对象，以及将其转化为真的对象存储到堆。方法被表示为简单的字节码集合（本质上是，“send"，”push literal“，和”return“）。
- 给予一个消息名和一个接送者，***lookup system***  决定查找的结果，例如，匹配槽。查找首先检查哈希表（代码表）找寻编译过的代码是否存在。如果存在，代码就被执行；否则，系统执行实际的对象查找（如果需要的化遍历接受者的父对象），然后调用编译器生成机器码来执行方法或者数据访问。
- ***compiler***  转换一个方法的字节码为机器码然后存储到***code cache***。 如果代码缓存中已经没有空间给新的编译过的方法，已经存在的方法会被清理来获得空间。代码缓存保存近似 LRU 信息来决定哪个编译过的方法被清理。
- 最后，虚拟机也包含许多的原语，可以被 SELF 程序调用来执行算术计算，I/O，图形绘制等。新的原语可以在运行时被动态链接到系统。

参考[88]，[115]，和[21]包含更多的系统细节。

##### 2.2.2 效率

因为 SELF 的纯语义威胁让程序效率极其低下，许多早期的实现尽力使用编译技术优化 SELF 程序。这些技术中有些是非常成功的：

***Dynamic compilation*** 。 编译器按需动态的翻译源方法成编译过的方法。这意味着没有分离的编译过程，执行和编译是交错的。（SELF 动态编译的使用灵感来自 Deutsch-Schiffman Smalltalk system[44]。）

***Customization***。 定制化允许编译器决定一个方法中许多消息接送者的类型[23]。它利用方法的许多消息是发送给 **self** 的事实来扩展动态编译。编译器根据每个接送者类型为一份源码创建不同版本的编译代码（Figure 2-1）。例如，方法min：源方法计算两个数的最小值，会被创建为两个不同的方法。这个复制允许编译器客制化每个版本到特定的接受者类型。特别是，在编译时知道 self 的类型使得编译器可以内联所有的发送 self。客制化在 SELF 中是非常重要的，因为许多消息是发送给 self，包括实例变量访问，全局变量访问，和许多用户定义的控制类型。

{% if 1 == 1 %} 
  {% asset_img figure_2_1.png title %}
{% else %}
  ![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_2_1.png)

{% endif %}

***Type Prediction***。确定的消息几乎只发送给特定的接受者类型。对于这样的消息，编译器使用最早由 Smalltalk 系统引入的优化[44，132]：它预测接受者的类型基于消息名字并且插入运行时类型检测于消息发送前用于测试期望的接送者类型（Figure 2-2）。类似的优化在 Lisp 系统中被用于优化通用计算。沿着类型测试成功的分支，编译器有关于接受者类型的精确信息可以静态地绑定和内联一个消息的拷贝。类如，现有的 SELF 和 Smalltalk 系统预测 ‘+’ 会被发送给整形 [128，58，44]，因为测量显示 这个类型 90% 的时间会出现 [130]。如果测试的开销是比较低且成功的可能性比较高，则类型预测会提高性能。

{% if 1 == 1 %} 
  {% asset_img figure_2_2.png title %}
{% else %}
  ![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_2_2.png)

{% endif %}

***Splitting*** 是另外一种方法，将一个多态的消息转变为多个分离的单态消息。通过拷贝部分的控制流图 [23，22，24] 避免类型测试。例如，一个对象在 if 语句的一个分支上是一个整形，而在另一个分支上是浮点（Figure 2-3）。如果这个对象是 if 语句之后的消息发送的接收者，编译器可以拷贝这个发送到两个分支。因此在每个分支上都是确定的接受者类型，编译器可以分别内联这两个发送的拷贝。

{% if 1 == 1 %} 
  {% asset_img figure_2_3.png title %}
{% else %}
  ![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_2_3.png)

{% endif %}

合并在一起，这些优化将 SELF的性能提升到合理的水平。举个例子，Chambers 和 Ungar 报告了 SELF 显著地跑赢了 ParcPlace Smalltalk-80 的实现在一个小的类C的整形测试用例集上[26]。

##### 2.2.3 源码级的语义

语言和实现特性的结合确保了所有程序的行为，即使错误的部分，也可以通过源码级别的术语独立理解。语言保证了每个消息发送要么找到一个匹配槽（访问它的数据或者运行它的方法），或者导致一个“message not understood" 的错误。最后，这个级别的唯一错误是查找错误。此外，实现保证了所有的原语是安全的。因为所有的原语会检查它们的操作数和结果，如果操作数无法被执行会以明确的方式失败。例如，所有的整型算术原语检查溢出，数组访问原语检查范围错误。最后，因为 SELF 不允许指针算术和使用垃圾回收，所以系统是指针安全的：不会偶然地复写随机的内存区域或者解引用悬挂指针。这些特性的结合让它更容易找到程序错误。

安全原语让高效实现更难[21]。例如，每个整型加法不得不检查溢出，从让它变慢。更重要的是，整型操作的返回值类型是未知的：所有的原语都有 “故障块” 的参数，如果操作失败，一个消息会被发送给这个参数。然后这个发送的结果会变成原语调用的返回值。例如，当前 SELF 系统的整型的 “+” 方法调用 IntAdd：带有失败块的原语会将失败块参数转化为任意精度的整型然后进行相加。因此表达式 x + y 的精确的返回值类型是未知的，即使 x 和 y 都是已知的整型：没有溢出，返回值会是整型，但是当结果太大而不能被表示为一个机器级别的整数时，结果就会时任意精度的整数。因此，即使 x 和 y 是已知的整数，编译器也无法静态的知道表达式 x + y + 1 的第二个 “+” 会调用整型的加法还是任意精度数的加法。安全原语帮助程序员的同时也潜在地使执行变慢。

##### 2.2.4 直接执行语义

SELF 通过使用动态编译模仿解释器。每当一个没有相关编译过代码的源码方法被调用，编译器会被自动调用生成缺失的代码。反过来，每当用户改变源方法，所有依赖于旧定义的编译过代码会被无效化。为了实现这个系统在源码方法和编译过的方法维持一个依赖链接[71,21]。

因为没有显式的编译和链接步骤，传统的编写-编译-链接-运行循环被简化为一个编写-运行循环。程序可以在运行时被改变，所以应用可以被在线调试而不需要从头开始运行。

#### 2.3 编译过程概述

Figure 2-4 较详细地展示了新的 SELF-93 系统的编译过程。SELF 的方法源码以对象的形式存储到堆上，就像其它的数据对象一样。方法的代码被编码为一个简单栈式机器的一串字节码（“instruction”）。这些字节码可以在解释器上直接执行；但是当前的 SELF 系统从没有这样做。反而，这些字节码总是按需地被翻译为机器码。当一个源码第一次被执行，机器码被实时生成。通常，第一次编译的代码是由“快速但笨"的编译器生成的。未优化的代码是字节码的直接翻译所以相当的慢；章节4 更细节地描述非优化的编译器。

{% if 1 == 1 %} 
  {% asset_img figure_2_4.png title %}
{% else %}
  ![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_2_4.png)

{% endif %}

未优化的代码包含一个调用计数器。每当计数器超过一定的阈值，系统会调用重编译系统来决定是否优化是需要的以及应该重新编译哪些方法。然后重编译系统调用优化编译器。章节5详细的解释了重编译过程，章节6描述优化编译器。

作为我们工作的一部分，我开发了一个新的内联缓存的变种，多态内联缓存（polymorphic inline caches(PICs)）。PICs 不仅加速多态调用点的运行时分配，也为编译器提供接收者类型的值信息。章节3描述PICs怎么工作的，章节5解释了优化编译器怎么利用 PICs 中的类型反馈信息。

在一些例子中，优化方法会再被重新编译。例如，重编译系统可以因为第一次优化后的方法仍然包含许多可以被内联的调用（比如，第一次优化编译的时候没有足够多的类型信息而不能内联）而决定重新编译。一个优化过的方法也会被重新编译当它遇到 “uncommon cases"：编译器可能预测某些场景从不会出现（如，整型溢出）而忽略了这种场景的代码生成。如果这种被忽略的场景刚好出现，原始优化过的方法就会被重新编译，扩展成处理这些特殊场景的代码（章节6.1.4）。

在本文的剩余部分，我们会按通常的顺序讨论系统的每个组件：首先，多态内联（PICs）和非内联编译器，然后重编译系统，最后是优化编译器本身。

#### 2.4 本工作的收益

本文所描述的所有技术都在 SELF-93 系统中实现了，在如下几个方面有了提升：

- ***Higher performance on realistic programs***。通过自动重编译和程序关键性能段的重新优化，我们的优化编译器可以利用运行时收集的信息。这些额外的信息经常使得编译器产生比之前代码更有效率的代码，即使编译器只执行很少的编译时程序分析。例如，DeltaBlue 约束求解器，当前的代码比之前版本的优化编译器产生的代码快三倍（章节7）。
- ***More stable performance***。新的编译器更多的依赖于动态观察到的程序行为和更少的静态分析，因此不太会受到静态分析技术盲点的影响。脆弱的性能是前一代 SELF 编译器的主要问题：小的源文件改变会导致戏剧性的性能损失，因为改变可能会导致一个特别重要的优化失效。例如，Stanford ineger  benchmarks 的一组小改变在前一代 SELF 编译器编译下会减慢 2.8 倍，而在新的编译器下编译只降低 16%（见章节7.4）。
- ***Faster and simpler compilation***。我们的方法依赖于运行时系统的动态反馈而不是使用复杂的静态类型分析技术在更好的运行时性能上有额外的优势。首先，它导致一个简单的编译器（11000 对比 26000 非注释源行代码）。第二，新的编译器比之前的编译器快2.5倍（见9.5节）。对于用户，编译暂停降低了更多（见9.2节）因为系统只是优化一个应用的关键路径。
- ***Support for source-level debugging***。在之前的 SELF 系统，用户可以打印优化程序的栈，但是他们不能在运行的时候改变程序，还有他们也不能执行诸如单步调试等通用的调试手段。我们的新系统提供随时的退优化功能让调试员的工作更简单（见第十章节）。

#### 2.5 相关工作

本节比较 SELF 系统和常见的相关工作；后续章节给出更多的比较细节。

##### 2.5.1 动态编译

SELF 使用动态编译，例如，运行时实时生成代码。动态生成编译代码而不是使用传统的批量式编译的思路源自于快速解释器的需求不断增加；通过编译到本地代码，一些解释器的固定开销（特别是伪指令的解码）可以被避免。例如，假设变量的类型保存不变，Mitchell[97] 建议可以将动态类型解释式程序的一部分转为编译形式。编译过的代码第一次被生成是作为一个表达式解释时的副作用产生的。类似地，线程代码[12]最早被用于消除一些解释过程的固定开销的。

传统上使用解释器或者动态编译器有两个原因：第一，一些语言很难被有效的静态编译，通常是因为独立的源程序没有足够的低级别的类型实现信息来生成高效的代码。第二，一些语言过于强调交互使用因此被实现为解释器而不是慢速编译器。

APL 是一种既难于进行静态编译（因为许多操作是多态的）且强调交互使用的语言。不出意外的话，APL 系统是最早探索动态编译的系统之一。例如，Johnston[80] 描述了一个使用动态编译作为解释的一个高效替代的 APL 系统。这些系统使用的一些机制类似于客制化（如，Guibas and Wyatt[61]）和内联缓存（Saal and Weiss[111]）。

Deutsch 和 Schiffman 在面向对象语言开创性地使用了动态编译。他们的 Smalltalk-80 的实现动态翻译了 Smalltalk 虚拟机定义的字节码到本地机器码并进行缓存供后续使用；Deutsch 和 Schiffman 估计仅仅使用简单的动态编译替换解释就加速了他们的系统 1.6 倍，使用更复杂的编译器则会有将近 2 倍的收益。

Franz[55] 描述了一个在加载时将紧凑的中间代码表示转为机器码的动态编译的变种。就像 Smalltalk ”snapshot“ 中的字节码，中间表示代码是架构独立的，同时，它也尽量是语言独立的（当前的实现只有Oberon支持）。从中间码到机器码的编译是足够快的使得当前加载器与常规加载器相比差异不大。

动态编译除了语言实现之外对于应用也是有用的[82]，它已经在多种方式下被使用。例如，Kessler et al. 用它来实现调试器的快速断点[84]。Pike et al. 通过动态生成最佳代码序列来加速 "bit-blt" 图形原语[103]。在操作系统中，动态编译被用来高效支持细粒度并行[32，105]和消除协议栈的开销[1]，以及动态链接[67]。动态编译也在其它领域被使用，如数据库查找优化[19，42]，微代码生成[107]，快速指令集仿真[34，93]。

##### 2.5.2 客制化

客制化部分编译代码到特定环境的想法与动态编译密切相关因为环境信息直到运行前不总是有效的。例如，Mitchell 的系统[97] 特殊化算术操作到操作数的运行时类型。当变量的类型改变了，所有的依赖于这个类型的编译代码都会被丢弃掉。因为这个语言不支持用户自定义多态且不是面向对象，这个方案的主要动机是降低解释过程的固定开销和将一些通用的内置操作替换为简单的，特定的代码序列（如，用整型加法替换通用加法）。

相似地，APL 编译器为某些表达式创建特定的代码[80，51，61]。对于这些系统，HP APL 编译器[51]最接近 SELF 使用的客制化技术。HP APL/3000 系统按一条一条语句的方式编译代码。除了执行 APL 特定的优化，编译过的代码被特定化根据特定的操作数类型（维数，每一维的大小，元素类型，存储布局）。这些所谓的”硬“代码比起通用的版本可以更高效地执行因为一个APL操作执行的计算可能会因实际参数类型而有很大差异。为了保持语言的语义，在编译表达式的时候，特殊代码之前会有一段验证导言（prologue）保证参数类型确实符合之前给出特定的假设。因此，编译生成的代码可以在之后的表达式执行中安全地重复使用。如果类型是相同的（希望这是常见的情况），无需在进一步编译；如果类型不同，一个新的更少约束性假设的版本会生成（所谓的“软"代码）。从系统的描述中，老的”硬“代码是否会保留是不清楚的，多个硬版本是否可以同时存在也是不清楚的（不同场景的特定化版本）。

客制化也被用于传统的，面向批处理的编译器。例如，Cooper et al. 描述一个 FORTRAN 编译器会创建程序的客制化版本来使能某个循环优化[36]。Nicolau 描述了一个 FORTRAN 编译器会动态地选择一个合适的静态生成的循环版本[99]。Saltz et al. 延迟循环调度直到运行时[112]。Przybylski et al. 为不同的模拟参数集合生成特定的缓存模拟器[104]。Keppel et al. 讨论为不同的应用生成值特化的特定化运行时[83]。在许多面向理论的计算机科学领域，客制化被称为局部求值[15]。

##### 2.5.3 先前的 SELF 编译器

本论文描述的 SELF 编译器有两个前代。第一个 SELF 编译器[ 22，23] 引入客制化和分离，取得相当不错的性能。对于 Stanford integer benchmarks 和 Richards benchmark，程序运行比优化过的 C 慢 4-6 倍，同时比最快的 Smalltalk-80 系统快两倍。编译器的中间码是基于树的，这让它难以显著地提高代码质量因为没有显式的控制流表示。例如，难以找到给定节点的后继（控制流术语），这让跨几个节点的优化难以实现。编译速度通常很好但会有很大的波动因为有些编译器算法的编译时间与源代码大小不是线性的。但是，编译暂停很明显，因为所有的源码都会被全量优化。这个编译器用了 9500 行 C++ 实现。

第二个编译器[21,24]（叫 SELF-91）被设计来移除第一个编译器的一些限制且进一步提高了运行时性能。它引入了迭代类型分析和一个更复杂的后端。因此，它在 Stanford integer benchmarks 上取得显著的性能（达到了优化过的C的一半性能）；但是，在 Richards benchmark 上的性能没有显著地提高。不幸的是，用户也在他们的程序上遇到相同的差异：一些程序（特别是小整型循环）表现为良好的性能的同时，许多大程序表现并不好（我们会在7.2节详细地讨论这个差异的原因）。编译器执行复杂的分析和优化也带来了编译时间的开销：比起前一代 SELF 编译器，编译慢了几倍。且当前的编译暂停会导致更多的用户不愿意使用新的编译器。比起前一代的编译器，SELF-91 编译i器相当复杂，用了 26000 行 C++代码实现。

##### 2.5.4 Smalltalk-80 编译器

Smalltalk-80 可能是最接近 SELF 的面向对象的语言了，有几个项目在调研加速 Smalltalk 程序的技术。

###### 2.5.4.1 Deutsch-Schiffman 系统

Deutsch-Schiffman 系统[44]代表着最先进的商业 Smalltalk 实现。它包含了一个简单但快速的动态编译器只执行窥孔优化没有内联。但是，不像 SELF，Smalltalk-80 的实现硬编码了某些重要的方法，如整型加法，消息实现的 if 语句，和某些循环。因此，Deutsch-Schiffman 编译器可以为这些结构生成高效的代码，否则只能通过类似于 SELF 编译器做过的优化来实现。Deutsch-Schiffman 编译器大约使用 50 条指令来生成一条编译过的机器指令[45]，因此编译器暂停几乎不可见。

除了动态编译，Deutsch-Schiffman 系统还开创了几项优化技术。内联缓存通过缓存最后的查找结果加速消息查找（见第三章）。因此，许多发送的开销可以降低到一个调用的开销和一个类型测试的开销。SELF 也使用内联缓存，且多态内联缓存扩展了它的用处到多态调用点（见第三章）。类型预测加速了普通发送通过预测可能的接送者类型（见 2.2.2节）。所有的 SELF 编译器除了第四章描述的非优化的 SELF 编译器都不同程度的使用了类型预测。优化的 SELF 编译器通过基于类型反馈信息的动态预测接收者类型扩展了静态类型预测（第五章）。

###### 2.5.4.2 SOAR

SOAR("Smalltalk On A RISC") 项目通过软硬协同的方式加速了 Smalltalk[132，129]。在软件方面，SOAR 使用了非动态的本地代码编译器（如，所有的方法在解析之后都会被编译成本地代码），内联缓存，类型预测，和一个分代扫描垃圾回收器[131]。就像 Deutsch-Schiffman 编译器一样，SOAR 编译器不执行昂贵的全局优化或者方法内联。在硬件方面，SOAR 是 Berkeley RISC II 处理器的变种[98]；最重要的硬件特性是寄存器窗口和标记整型指令。比起其它的 Smalltalk 在 CISC  上的实现，SOAR 软硬件特性的结合是非常成功的：在一个 400ns 的时间周期里，SOAR 与 70ns的微指令 Xerox Dorado 工作站，但比跑在周期时间为 200ns 的 Sun-3 上的 Deutsch-Schiffman Smalltalk 系统快 25%。但是，我们会在第八章中看到SELF系统使用的优化技术极大地降低了特殊支持硬件的性能收益。

###### 2.5.4.3 其它 Smalltalk 编译器

其它的Smalltalk 系统尝试通过标注类型声明来加速程序。Atkinson 部分实现了一个 Smalltalk 编译器，其使用类型声明作为提示来生成更好的代码[11]。例如，如果程序员定义一个局部变量为 “class X" 类型，编译器会查找这个变量的消息发送然后内联调用方法。为了代码安全，编译器在内联代码之前插入了类型检查；如果当前的值不是 class X 的实例，一个非优化的，无类型的方法会被调用。虽然，Atkinson 的编译器一直不是完全的，它可以运行小的用例得到与 Sun-3 上的 Deutsch-Schiffman 系统相比，速度提高了大约两倍。当然，为了获得加速，程序员不得不小心地在代码里标注类型，只有调用使用了正确的类型才会被加速。

TS 编译器[79]使用了类似的方法。类型被指定为一组类，当接送者类型是单类编译器可以静态绑定调用。方法可以单一内联如果程序员标记它们是可以内联的。如果接受者类型是一小组类，编译器为每个类插入类型检查然后分别优化各个分支。这些优化可以被看做是类型预测的一种形式，它是基于程序员提供的类型声明，而不是编译器硬连接的消息列表和预期类型。但是，不像类型预测和Atkinson 的编译器的是，程序员的类型声明不是一种提示而是一种坚定的承承若（这些承诺的有效性通过类型检查器来检查，但是检查器一直式不完善的，因此无法分析 Smalltalk 系统的重要部分）。

TS 编译器的后端使用 RTL-base 中间形式进行粗放的优化。因此（还有它本身是用 Smalltalk 编写的），编译器非常的缓慢，编译一个非常小的 benchmark 需要 15 到 30 秒[79]。有效的关于高效代码生成的公开发布的数据非常少，因为 TS 编译器从没有完整过，只能编译非常小的 benchmark 程序。在 68020-base 的工作站上关于 TS 和 Tektronix Smalltalk 解释器的比较数据显示对于小 benchmark ，TS 是 Deutsch-Schiffman 系统的两倍快，这是标注了类型声明的TS[79]。但是比较不是完全有效的因为 TS 没有实现一些 Smalltalk结构的完全语义。例如，整型计算没有溢出检查。这个对于小的 TS benchmark 的性能会有显著影响。例如，sumTo benchmark的循环（从1到10000一次相加）只包含很少的指令因此溢出检查会有显著的开销。更重要的是，累积求和的结果的类型会无法用 SmallInteger 表示，因为 SmallInteger 运算的结果不一定就是 SmallInteger（当溢出发生，Smalltalk-80 原始故障代码将参数转换为任意长度的整数并返回这些参数的总和）。因此，求和的类型声明（上边界）不得不更通用，这个会显著降低生成的代码的运行速度。

### 3. 多态内联缓存

面向对象的程序发送许多的消息，因此发送必须很快。本章首先回顾动态类型语言里已经存在的知名的提高查找效率的技术，然后描述多态内联缓存，这是我们开发的一个标准内联缓存的扩展版本。

#### 3.1 离线查找缓存

面向对象语言用消息发送替换过程调用。发送动态绑定消息比起调用静态绑定过程花费更多因为程序必须根据接收者的类型和语言的继承规则查找正确的目标函数。虽然早期的 Smalltalk 系统有简单的继承规则和相对慢速的解释器，方法查找（也叫消息查找）仍然占据执行时间的大部分。

***Lookup caches*** 降低了动态消息绑定的固定开销。一个查找缓存映射了一组（接受者类型，消息名）到目标方法且保存了大部分最近使用的查找结果。消息发送首先用给定的接收者类型和消息名为索引查询缓存。当缓存探测失败它们才会调用（昂贵的）查找函数通过遍历继承图来查找目标函数，然后存储结果到查找缓存，可能会将比较老的查找结果替换掉。查找缓存能有效的降低查找的固定开销。例如，Berkeley Smalltalk，没有查找缓存会慢 37% [128]。

#### 3.2 调度表

静态类型的语言总是通过调度表实现消息查找。一个通用的变种是使用消息名（编码为整数）来索引特定类型消息表，这个表包含了目标函数的地址。然后一个消息发送会包含调度表的加载地址（被存放到每个对象的第一个字中），索引到这个表来获得目标函数的地址，然后调用这个函数。

调度表在静态类型语言中是容易实现的因为表索引的范围，发送给一个对象的可能的消息集合，是静态已知的，因此这些表的大小（和内容）是容易计算出来的。但是，在动态类型语言中也是可能可以使用调度表的，尽管需要一些加法计算[7,50,135]。这些方法的主要缺陷是它们难以被整合进一个交互系统因为当引入新的消息或者类型时，调度表需要定期地被从新排布；在一个大的系统如 Smalltalk-80，这种从新排布需要花费许多分钟，且调度表会消耗几百KB内存。因此，我们不对调度表做进一步的讨论。

#### 3.3 内联缓存

即使用了查找缓存，发送一条消息仍花费比调用一个简单过程的时间来得长因为许多消息发送都需要先探测缓存。即使在理想场景下，一个缓存查找关联了连续得十条指令来获得接送者类型和消息名，形成缓存索引（例如，通过异或和移位接收者类型和消息名），获得缓存条目，然后比较这个条目和实际的接受者类型和消息名来验证缓存的目标函数是确实是正确的。这个情况下，查找缓存是 100% 有效的（访问都命中），发送仍然相当地慢因为每次发送查找缓存的探测都要加上十条指令。

幸运地是，消息发送可以加速因为看到了在一个给定的调用调用点的接收者类型是很少变化的；如果在一个特定的调用点一个消息被发送给一个类型X的对象，下一次这个发送被执行大概率仍然是X类型的接收者。例如，Smalltalk 代码的几个研究表明，一个给定调用点的接收者类型 95% 的时间里是保持不变的[44，130，132]。

这种类型使用的局部性可以通过在调用点缓存要查找函数的地址来加以利用。因为查找结果被缓存“在线”在每个调用点（例如，在命中场景下不会访问单独的查找缓存），这个技术叫***inline cahcing***[44，132]。图 3-1 显示了空的内联缓存；调用函数简单的包含一个系统查找例程。第一次这个调用被执行，查找例程会找到目标方法。但是在跳到目标前，查找例程改变调用指令去执行刚找到的目标方法（图 3-2 ）。随后发送执行直接跳到目标方法，完全避开任何查找。当然，接收者的类型可以被改变，所以被调用方法的导语必须验证接收者类型是正确的，如果类型测试失败调用查找代码。

{% if 1 == 1 %} 
  {% asset_img figure_3_1.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_1.png)

{% endif %}

{% if 1 == 1 %} 
  {% asset_img figure_3_2.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_2.png)

{% endif %}

因为以下一些原因内联缓存比离线查找缓存快得多。首先，因为每个调用点都有分离的缓存，不需要测试消息名来验证缓存是否命中--只有在未命中的时候测试才必须做（通过系统查找例程）。第二，内联缓存不需要执行任何的加载指令来获取缓存条目；这个功能通过调用指令隐式地执行了。最后，因为没有显示地索引到任何表，我们可以忽略哈希函数的异或和移位指令。唯一的开销是接收者类型的检查通常用非常少的指令完成（在一个典型的 Smalltalk 系统是两个加载和一个比较跳转，在SELF系统是一个加载和一个常量比较）。

内联缓存在降低查找开销上非常的有效因为命中率高且命中开销小。例如，SOAR（一个RISC处理器上的Smalltalk实现）没有内联缓存会慢33% [132]。我们已知的所有的编译实现的Smalltalk都包含内联缓存，包括 SELF 系统。

#### 3.4 处理多态发送

仅当调用点的接收者类型（和调用目标）保持相对固定内联缓存才有效。虽然内联缓存对主要的发送都工作的很好，但是它不能加速有着几个接收者类型的多态调用点因为调用目标会在几个不同的方法来回切换。

例如，假设一个方法是发送 **显示** 消息到一个表中的所有元素，然后表中的第一个元素是长方形：

{% if 1 == 1 %} 
  {% asset_img rectangle.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\rectangle.png)

{% endif %}

如果表中下一个元素是圆形，控制通过内联缓存中的调用即长方行的显示方法。方法导语中的接收者测试检测到类型错误然后调用查找例程重新绑定内联缓存到圆形的显示方法：

{% if 1 == 1 %} 
  {% asset_img figure_3_3.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_3.png)

{% endif %}

不幸地是，如果表中包含的圆形和长方形不是特定的顺序，内联缓存会一次又一次地失败因为接收者类型总是改变。在这个例子中，内联缓存可能比使用离线缓存更慢因为失败次数太多了。特别是，在大部分呢现代处理器中失败代码其中包含改变调用指令需要无效化指令缓存的部分内容。通常，失败处理也包含相当大的额外开销。例如，SELF 系统链接所有的调用特定方法的内联缓存到一张表如此当方法被重定位到不同的地址或者被丢弃它们可以被更新。

内联缓存失败导致的性能影响在高性能系统中变得更严重，以至于不能在被忽视。例如，SELF-90 的评测显示 Richards benchmark 花费 25% 的时间来处理内联缓存失败[23]。

SELF 系统上的一个非正式的多态调用点测试显示大部分的用例多态的度都是比较小的，通常小于十。发送的多态性的度符合三峰分布：大部分的发送是单态的（只有一个接收者类型），部分是多态（少量的接收者类型），非常少的是巨态的（非常多的接收者类型）。图 3-4 显示非空内联缓存的数量分布，例如，调用点展示的多态性的度。这个分布是在使用原型 SELF 用户接口[28]一分钟后获得的。极大部分的调用点都只有一个接收者类型；这是通常的内联缓存也能工作得很好的原因。相当少的调用点有两个不同的接收者类型；一个常见的例子是布尔消息，因为对和错在SELF中是两个类型。极少的调用点有多于五个的接收者类型。（所有的超过十个接收者类型的调用点只有块；节4.3.2 解释了这个巨型态行为。）图 3-4 多态调用的性能可以通过一个更自由的缓存形式来提高因为大部分非单态的发送只有少量的接收者类型。

{% if 1 == 1 %} 
  {% asset_img figure_3_4.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_4.png)

{% endif %}

#### 3.5 多态内联缓存

为了降低内联缓存失败的开销，我们设计和实现了多态内联缓存，一钟内联缓存的新的扩展技术用来高效处理多态调用点。不仅仅是缓存最后的查找结果， 一个多态内联缓存（PIC）为一个给定的多态调用点缓存了几个查找结果到一个专门生成的桩例程里。

在我们发送显示消息到列表元素的例子中，一个系统初始化使用 PICs 就像正常的内联缓存一样：在第一次发送后，内联缓存绑定到长方形显示方法上。

{% if 1 == 1 %} 
  {% asset_img rectangle_2.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\rectangle_2.png)

{% endif %}

但是当遇到第一个圆形时，不是简单地切换调用目标到圆形的显示方法而是失败处理过程会构造一个短的桩例程然后重新绑定调用到这个短的例程上：

{% if 1 == 1 %} 
  {% asset_img figure_3_5.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_5.png)

{% endif %}

桩（类型匹配）检查接收者是长方形还是圆形然后跳转到对应的方法。这个桩可以直接跳转到方法体中（跳过方法导语的类型检查）因为接收者类型已经验证过了。然而，方法仍然需要导语的类型检查因为它们也会被单态调用点调用此时还是标准的内联缓存。

因为 PIC 当前缓存了长方形和圆形，如果列表里只有长方形和圆形就不会再有失败发生。所有的发送都是快速的，在找到目标前只需要一到两个比较。如果缓存失败再次发生（如接收者既不是长方形也不是圆形），桩例程会简单的扩展来处理新的场景。最终，桩会包含实践中的所有场景，不会再有缓存失败和查找。

##### 3.5.1 变化

上面描述的方案在大部分的场景下工作良好将多态发送开销降低到非常少的机器周期。这一节讨论一些固有问题和可能的解决方法。

***Copying with megamorphic sends***。一些发送点可能会发送一个消息到非常大的类型数量。例如，一个方法会发送 writeSnapshot 消息给系统的任意的对象。为这样的发送构建一个巨大的 PIC 浪费时间和空间。因此，内联缓存失败处理不会为超出一定类型数量用例扩展PIC；而是，标记这个调用点为巨态且采用反馈策略，可能只是传统的单态内联机制。

在 SELF 系统中，如果超出一定的大小（当前，10个） PICs 就标记为巨态。当一个巨态 PICs 未命中，它不会为了包含新的类型而增长。而是，其中一个场景会被随机选到然后被替换为新的场景。这个系统的一个早期版本使用 “move-to-front” 策略即在前面插入新的场景，往后移动其它的场景且去掉最后一个场景。但是，这个策略已经被放弃因为它的失败开销太大了（所有的10个条目都需要被改变，而不是只有一个）且它的失败率也有非常高的偶然性，例如类型循环的改变。（是的，墨菲定律是成立的--真实程序反应了这个现象）。

***Improving linear search***。如果每个类型的动态使用频率是已知的，PICs 可以定期重新排序从而将最频繁出现的类型放到 PIC 的开头，降低类型检查执行的平均次数。如果线性搜索不够高效，更复杂的算法如二分搜索或者某些形式的哈希可以被用于许多类型的场景。但是，因为类型的数量平均上是很小的（见图 3-4），这个优化可能不值得大力去做：对于大多数场景一个线性搜索的 PIC 是可能快于其它方法的。

***Improving space efficiency***。多态内联缓存是大于正常的内联缓存的因为桩例程关联了每个多态调用点。如果空间紧张，有着完全相同消息名的调用点是可以共享一个共同的 PIC 从而减少空间固有开销。在这个设想下，PICs 可以充当快速的特定消息查找缓存。一个多态发送的平均开销很可能高于特定调用点 PICs 因为每个 PIC 的类型数量会增加，这个是由于局部性丢失导致的（一个共享 PIC 将会包含特定消息名的所有接收者类型，调用相关的 PIC 只包含实际出现在调用点的类型）。

#### 3.6 实现和结果

我们为 SELF 系统设计和实现了多态内联缓存并且测量它们的效果。这节所有的测量都是在一个有 48 MB内存的轻量加载的 Sun-4/260 上进行的；用于比较的基础系统是1990 年 9 月的 SELF 系统。这个基础系统使用内联缓存；一个直到特定方法的代码的发送需要 8 条指令（9 个周期）。一个内联缓存失败花费 15 毫秒或者 250 周期。失败时间可以通过一些优化和用汇编重编码关键部分来降低。我们估计这些优化可以降低失败的固有开销两倍左右。因此，我们的测量可能会夸大 PICs 的直接性能优势到两倍左右。另一方面，商业的 Smalltalk-80 实现（ParcPlace Smalltalk-80 系统，2.4版本）的测量表明它花费 15 毫秒处理失败，所以我们当前的实现并不是看上去的那样无理由的慢。

我们实验的系统的单态发送使用了和基础系统相同的内联缓存形式。对于多态发送，一个桩被构造来测试接收者类型和跳转到对应的方法。这个桩有个固定的 8 周期开销（加载接收者类型和跳到目标方法），每个类型测试花费 4 周期。PICs 就像 3.5 节描述的那样实现的。前面章节提到的变化部分基本没有实现，除了调用点可能会被当做巨态当它超过十个接收者类型（但是这种调用没有出现在我们的 benchmarks）。

{% if 1 == 1 %} 
  {% asset_img Table_3_1.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\Table_3_1.png)

{% endif %}

表 3-1 总结了查找时间。基础系统里的多态发送开销依赖于多态调用点的内联缓存失败率。例如，如果发送执行每三次更改一次接收者类型，花费将会接近于 250/3 = 83 周期。对于有 PICs 的系统，平均多态查找开销是用例数量的函数；如果所有的 n 个用例都是完全相似的，平均分发将会涉及 n / 2 类型测试所以花费 8 + （n / 2）* 4 = 8 + 2n 周期。

为了评估多态内联缓存的效果，我们测量了一套 SELF 程序。这个程序（有 PolyTest 的异常）可以被认为是相当典型的面向对象的程序能覆盖各种编程风格。更多的 benchmarks 数据在 [70] 中给出。

  **Parser**。一个递归向下的解析器为早期 SELF 语法版本开发的（550行）。

  **PrimitiveMaker**。根据原语描述生成 C++ 和 SELF 桩例程的程序（850行）。

  **UI**。SELF 用户接口原型（3000行）运行一个小的交互界面。因为用于我们的测试的 Sun-4 没有特殊的图形硬件，运行中占主导地位的是图形原语（如多边形填充和全屏位图拷贝）。对于我们的测试，有三个非常昂贵的图形原语被转为无操作；保留的原语仍然占据全部运行时间的 30%。

**PathCache**。SELF 系统的一部分，用于计算所有全局对象的名字然后存储为压缩格式（150行）。大部分时间花在一个循环上其迭代处理一个包含 8 个不同种类对象的集合。

**Richards**。一个操作系统模拟 benchmark （400行）。这个 benchmark 调度执行四个不同的任务。它包含了一个经常执行的多态发送（调度器发送 RunTask 消息到下一个任务）。

**PolyTest**。一个人造的 benchmark（20行）为了显示 PICs 所能达到的最高加速而设计的。PolyTest 由一个 5 度多态发送的循环组成；发送被执行一百万次。正常的内联缓存在这个 benchmark 会有 100% 的失败率（不存在两个递归发送有相同的接收者类型）。因为 PolyTest 是比较小的，人造的 benchmark，所以计算整个 benchmark 集的平均值的时候不会包含它。

##### 3.6.1 执行时间

为了获得更准确的测量，所有的 benchmark 连续运行 10 次然后计算平均的 CPU 时间。这个程序重复10次，选择最小的平均值（假设比较长的时间是因为其它的 UNIX 程序导致的）。一次垃圾回收被执行在每次测量前为了减少不准确性。图 3-6 显示系统使用 PICs 节省的执行时间（详见附录A中的表 A-2 ）。

{% if 1 == 1 %} 
  {% asset_img figure_3_6.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_6.png)

{% endif %}

节省的时间从无（UI）到重大的（Richards，Parser）再到壮观的（PolyTest）；benchmark 节省的时间的中位数是 11% （去掉 PolyTest）。从单个 benchmarks 观察到的加速与基础系统中处理内联缓存失败的时间紧密相关。例如，在基础系统的 PolyTest 花费了超过 80% 的执行时间在失败处理上，然后超过 80% 的执行时间由 PICs 消除。这种密切的相关性表明 PICs 几乎消除了内联缓村失败的所有固有开销。

{% if 1 == 1 %} 
  {% asset_img Table_3_2.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\Table_3_2.png)

{% endif %}

表 3-2 显示了我们的 benchmarks 在基础系统的失败率。虽然 SELF 的实现有自己的优化编译器与 Smalltalk 系统完全不同，但与之前那些 Smalltalk 系统的研究观察到的失败的概率相同的，其观察到的失败概率大约是 5% [44，130，132]。当引入 PICs 时失败率不与加速直接相关因为 benchmark 有完全不同的调用频率（相差五倍以上）。

一个可能的期望是内联缓存失败率相关于程序中存在的多态机制的度，通过多态调用点的消息发送的分数来衡量（如，来自内联缓存即遇到了至少两个不同的接收者类型）。例如，一个假设是一个程序发送的80%的消息来自多态调用点有更高的内联缓存失败率比起一个程序只发送 20% 来自多态调用点的消息。有趣的是，我们的 benchmark 程序中没有这样的场景（图 3-7）。例如，PathCache 有超过 73% 的消息发送来自多态调用点而 Parser 有 24%，但是 PathCache 的内联缓存失败率（5.8%）稍低于 Parser 的失败率（6.2%）。PathCache 中只有一个接收者类型在大部分多态调用点中占主导地位（所以接收者类型基本不改变），而 Parser 的内联缓存的接收者类型频繁地改变。因此，按发送频率对 PICs 类型测试进行排序（如 3.2 节的建议）对于像 PathCache 的程序可能是个胜利。

{% if 1 == 1 %} 
  {% asset_img figure_3_7.png title %}
{% else %}
![](H:\Blogs\fiking\source\_posts\Adaption-Optimization-for-SELF-Reconciling-high-performance-with-exploratory-programming-论文翻译\figure_3_7.png)

{% endif %}

##### 3.6.2 空间固有开销

PICs 的空间固有开销是比较低的，通常小于编译代码的 2%（见 表 3-3）。低固有开销的主要原因是大部分调用点不需要 PIC 因为它们是单态的。因此，即使一个 2 个元素的 PIC 桩占据超过了 100 字节，整体空间开销仍非常适中。

#### 3.7 总结

传统的内联缓存对于大部分的发送都工作良好。 但是，真的多态调用点（例如，调用点的接收者类型改变的比较频繁）会导致显著的内联缓存开销，我们测量下来要消费 25% 的程序总执行时间。

我们用多态内联缓存（PICs）扩展传统的内联缓存从而可以缓存多个查找目标。PICs 非常有效地移除了内联缓存的固有开销，加速了我们的 benchmark的执行达到 25%，中位数是 11%。

虽然我们已经讨论了 PICs 作为加速消息发送的一种方法，但它在SELF系统中的主要目的是为了优化编译器提供类型信息。通过这种类型信息获得的性能提升远远超过本章中观察到的加速。我们将会在第五章详细讨论 PICs 的这方面内容。

### 4. 非内联编译器



# 总结

